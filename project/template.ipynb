{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, weâ€™ll be using the [Women's Ecommerce Clothing Reviews Dataset from Kaggle](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews), which Kaggle states is a \n",
    "\n",
    "> dataset revolving around the reviews written by customers. Its nine supportive features offer a great environment to parse out the text through its multiple dimensions. Because this is real commercial data, it has been anonymized, and references to the company in the review text and body have been replaced with â€œretailerâ€. \n",
    "\n",
    "The machine learning task will be sentiment analysis, classifying each review as having positive or negative sentiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Training and Evaluating Sentiment Analysis Models Using Metaflow\n",
    "\n",
    "In this task, you'll use Metaflow to build two machine learning models for sentiment analysis: a baseline *\"majority class\"* classifier and your own custom model. You'll then train both models in parallel and experiment with different hyperparameters to optimize their performance. Finally, you'll use this notebook and the Metaflow Client API to analyze the results of your different models and hyperparameters. Here's what you'll need to do:\n",
    "\n",
    "### Step 1: Build the Workflows\n",
    "The first step in this task is to build the workflow(s) for sentiment analysis using the Metaflow framework. Start by creating a new flow in Metaflow and implementing the baseline *\"majority class\"* classifier. Then, build your own custom classifier using techniques you learned in Week 1, or any [helpful resources](https://outerbounds.com/docs/nlp-tutorial-L2/) you'd like. For your custom model, be sure to include steps for data preprocessing, model training, and evaluation.\n",
    "\n",
    "### Step 2: Train Both Models in Parallel\n",
    "Once you've built your models, the next step is to train both models in parallel using the Metaflow framework. Use Metaflow to run both training jobs in parallel steps. If you get stuck, you may want to review the [FlowSpec branching documentation](https://docs.metaflow.org/metaflow/basics#branch).\n",
    "\n",
    "### Step 3: Experiment with Hyperparameters\n",
    "After you've trained both models in parallel, the next step is to experiment with different hyperparameters to optimize their performance. Try different values for hyperparameters such as learning rate, batch size, and number of epochs, and record the results for each combination of hyperparameters as Data Artifacts in Metaflow.\n",
    "\n",
    "### Step 4: Analyze the Results\n",
    "Finally, use this notebook and the Metaflow Client API to analyze the results of your different models and hyperparameters. Create visualizations to compare the performance of the two models and identify the best hyperparameters for each one.\n",
    "\n",
    "By completing this task, you'll gain experience working with the Metaflow framework and learn how to build and optimize machine learning workflows for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# You can style your plots here, but it is not part of the project.\n",
    "YELLOW = '#FFBC00'\n",
    "GREEN = '#37795D'\n",
    "PURPLE = '#5460C0'\n",
    "BACKGROUND = '#F4EBE6'\n",
    "colors = [GREEN, PURPLE]\n",
    "custom_params = {\n",
    "    'axes.spines.right': False, 'axes.spines.top': False,\n",
    "    'axes.facecolor':BACKGROUND, 'figure.facecolor': BACKGROUND, \n",
    "    'figure.figsize':(8, 8)\n",
    "}\n",
    "sns_palette = sns.color_palette(colors, len(colors))\n",
    "sns.set_theme(style='ticks', rc=custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_function(row, cutoff=4):\n",
    "    \"\"\"\n",
    "    A function to derive labels from the user's review data.\n",
    "    \"\"\"\n",
    "    rating = row[\"rating\"]\n",
    "    binarized_rating = 1 * (rating > cutoff)\n",
    "    return binarized_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the data. \n",
    "file_path = \"/home/workspace/workspaces/full-stack-ml-metaflow-corise-week-1/data/Womens Clothing E-Commerce Reviews.csv\"\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# transformations\n",
    "df.columns = [\"_\".join(name.lower().strip().split()) for name in df.columns]\n",
    "df = df[~df.review_text.isna()]\n",
    "df['review'] = df['review_text'].astype('str')\n",
    "_has_review_df = df[df['review_text'] != 'nan']\n",
    "reviews = _has_review_df['review_text']\n",
    "labels = _has_review_df.apply(labeling_function, axis=1)\n",
    "df = pd.DataFrame({'label': labels, **_has_review_df})\n",
    "\n",
    "# split into training and validation.\n",
    "_df = pd.DataFrame({'review': reviews, 'label': labels})\n",
    "traindf, valdf = train_test_split(_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindf and valdf reference _df -- make them separate dataframes\n",
    "traindf = traindf.copy()\n",
    "valdf = valdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5522190328990947\n",
      "AUCROC: 0.5\n"
     ]
    }
   ],
   "source": [
    "# TODO: build the majority class baseline model. \n",
    "# TODO: find the majority class in the labels. ðŸ¤”\n",
    "# TODO: score the model on valdf with a 2D metric space: sklearn.metrics.accuracy_score, sklearn.metrics.roc_auc_score\n",
    "    # Documentation on suggested model-scoring approach: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "majority_class = traindf[\"label\"].mode()[0]\n",
    "valdf.loc[:, \"baseline_pred\"] = majority_class\n",
    "valdf.loc[:, \"baseline_score\"] = majority_class    # score is probability of class 1. If majority class is 0, prob(1) = 0, otherwise prob(1) = 1\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true=valdf[\"label\"], y_pred=valdf[\"baseline_pred\"]))\n",
    "print(\"AUCROC:\", roc_auc_score(y_true=valdf[\"label\"], y_score=valdf[\"baseline_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "# TODO: modify this custom model to your liking. Check out this tutorial for more on this class: https://outerbounds.com/docs/nlp-tutorial-L2/\n",
    "# TODO: train the model on traindf.\n",
    "# TODO: score the model on valdf with _the same_ 2D metric space you used in previous cell.\n",
    "# TODO: test your model works by importing the model module in notebook cells, and trying to fit traindf and score predictions on the valdf data!\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class NbowModel():\n",
    "    def __init__(self, vocab_sz):\n",
    "\n",
    "        self.vocab_sz = vocab_sz\n",
    "\n",
    "        # Instantiate the CountVectorizer\n",
    "        self.cv = CountVectorizer(\n",
    "            min_df=.005, max_df = .75, stop_words='english', \n",
    "            strip_accents='ascii', max_features=self.vocab_sz\n",
    "        )\n",
    "\n",
    "        # Define the keras model\n",
    "        inputs = tf.keras.Input(shape=(self.vocab_sz,), \n",
    "                                name='input')\n",
    "        x = layers.Dropout(0.10)(inputs)\n",
    "        x = layers.Dense(\n",
    "            15, activation=\"relu\",\n",
    "            kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)\n",
    "        )(x)\n",
    "        predictions = layers.Dense(1, activation=\"sigmoid\",)(x)\n",
    "        self.model = tf.keras.Model(inputs, predictions)\n",
    "        opt = optimizers.Adam(learning_rate=0.002)\n",
    "        self.model.compile(loss=\"binary_crossentropy\", \n",
    "                           optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        print(X.shape)\n",
    "        print(X[0])\n",
    "        res = self.cv.fit_transform(X).toarray()\n",
    "        all_fit_params = {\"batch_size\": 32, \"epochs\": 10, \"validation_split\": .2, **fit_params}\n",
    "        self.model.fit(x=res, y=y, **all_fit_params)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        print(X.shape)\n",
    "        print(X[0])\n",
    "        res = self.cv.transform(X).toarray()\n",
    "        return self.model.predict(res)\n",
    "    \n",
    "    def eval_acc(self, X, labels, threshold=.5):\n",
    "        return accuracy_score(labels, \n",
    "                              self.predict(X) > threshold)\n",
    "    \n",
    "    def eval_rocauc(self, X, labels):\n",
    "        return roc_auc_score(labels,  self.predict(X))\n",
    "\n",
    "    @property\n",
    "    def model_dict(self): \n",
    "        return {'vectorizer':self.cv, 'model': self.model}\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, model_dict):\n",
    "        \"Get Model from dictionary\"\n",
    "        nbow_model = cls(len(\n",
    "            model_dict['vectorizer'].vocabulary_\n",
    "        ))\n",
    "        nbow_model.model = model_dict['model']\n",
    "        nbow_model.cv = model_dict['vectorizer']\n",
    "        return nbow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting baseline_challenge.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile baseline_challenge.py\n",
    "# TODO: In this cell, write your BaselineChallenge flow in the baseline_challenge.py file.\n",
    "\n",
    "from metaflow import FlowSpec, step, Flow, current, Parameter, IncludeFile, card, current\n",
    "from metaflow.cards import Table, Markdown, Artifact, Image\n",
    "import numpy as np \n",
    "from dataclasses import dataclass\n",
    "\n",
    "label_cutoff = 4\n",
    "labeling_function = lambda row: 1 * (row[\"rating\"] > label_cutoff) # TODO: Define your labeling function here.\n",
    "\n",
    "@dataclass\n",
    "class ModelResult:\n",
    "    \"A custom struct for storing model evaluation results.\"\n",
    "    name: None\n",
    "    params: None\n",
    "    pathspec: None\n",
    "    acc: None\n",
    "    rocauc: None\n",
    "\n",
    "class BaselineChallenge(FlowSpec):\n",
    "\n",
    "    split_size = Parameter('split-sz', default=0.2)\n",
    "    data = IncludeFile('data', default='/home/workspace/workspaces/full-stack-ml-metaflow-corise-week-1/data/Womens Clothing E-Commerce Reviews.csv')\n",
    "    kfold = Parameter('k', default=5)\n",
    "    scoring = Parameter('scoring', default='accuracy')\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "        import pandas as pd\n",
    "        import io \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # load dataset packaged with the flow.\n",
    "        # this technique is convenient when working with small datasets that need to move to remove tasks.\n",
    "        df = pd.read_csv(io.StringIO(self.data))\n",
    "        # TODO: load the data. \n",
    "        # Look up a few lines to the IncludeFile('data', default='Womens Clothing E-Commerce Reviews.csv'). \n",
    "        # You can find documentation on IncludeFile here: https://docs.metaflow.org/scaling/data#data-in-local-files\n",
    "\n",
    "        # filter down to reviews and labels \n",
    "        df.columns = [\"_\".join(name.lower().strip().split()) for name in df.columns]\n",
    "        df = df[~df.review_text.isna()]\n",
    "        df['review'] = df['review_text'].astype('str')\n",
    "        _has_review_df = df[df['review_text'] != 'nan']\n",
    "        reviews = _has_review_df['review_text']\n",
    "        labels = _has_review_df.apply(labeling_function, axis=1)\n",
    "        self.df = pd.DataFrame({'label': labels, **_has_review_df})\n",
    "\n",
    "        # split the data 80/20, or by using the flow's split-sz CLI argument\n",
    "        _df = pd.DataFrame({'review': reviews, 'label': labels})\n",
    "        self.traindf, self.valdf = train_test_split(_df, test_size=self.split_size)\n",
    "        print(f'num of rows in train set: {self.traindf.shape[0]}')\n",
    "        print(f'num of rows in validation set: {self.valdf.shape[0]}')\n",
    "\n",
    "        self.next(self.baseline, self.model)\n",
    "\n",
    "    @step\n",
    "    def baseline(self):\n",
    "        \"Compute the baseline\"\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "        self._name = \"baseline\"\n",
    "        params = \"Always predict the majority class: \"\n",
    "        pathspec = f\"{current.flow_name}/{current.run_id}/{current.step_name}/{current.task_id}\"\n",
    "        \n",
    "        majority_class = self.traindf[\"label\"].mode()[0]\n",
    "        params += str(majority_class)\n",
    "\n",
    "        val_pred = [majority_class] * len(self.valdf)\n",
    "        val_score = [majority_class] * len(self.valdf)\n",
    "        predictions = val_pred # TODO: predict the majority class\n",
    "        acc = accuracy_score(self.valdf[\"label\"], val_pred) # TODO: return the accuracy_score of these predictions\n",
    "         \n",
    "        rocauc = roc_auc_score(self.valdf[\"label\"], val_score) # TODO: return the roc_auc_score of these predictions\n",
    "        self.result = ModelResult(\"Baseline\", params, pathspec, acc, rocauc)\n",
    "        self.next(self.aggregate)\n",
    "\n",
    "    @step\n",
    "    def model(self):\n",
    "\n",
    "        # TODO: import your model if it is defined in another file.\n",
    "        from model import NbowModel\n",
    "\n",
    "        self._name = \"model\"\n",
    "        # NOTE: If you followed the link above to find a custom model implementation, \n",
    "            # you will have noticed your model's vocab_sz hyperparameter.\n",
    "            # Too big of vocab_sz causes an error. Can you explain why? \n",
    "            # > Because of linear layer. Linear layer assumes `vocab_sz` number of input features. \n",
    "            # > If we set `vocab_sz` to a very large value, CountVectorizer doesn't see that many unique words, \n",
    "            # > and returns lesser number of features. \n",
    "            # > (Like linear layer expects 1 million features but CountVectorizer gives only 800 features!)\n",
    "        self.hyperparam_set = [{'vocab_sz': 100}, {'vocab_sz': 300}, {'vocab_sz': 500}]  \n",
    "        pathspec = f\"{current.flow_name}/{current.run_id}/{current.step_name}/{current.task_id}\"\n",
    "\n",
    "        self.results = []\n",
    "        for params in self.hyperparam_set:\n",
    "            model = NbowModel(params[\"vocab_sz\"]) # TODO: instantiate your custom model here!\n",
    "            model.fit(X=self.traindf['review'].to_numpy(), y=self.traindf['label'].to_numpy())\n",
    "            acc = model.eval_acc(X=self.valdf[\"review\"].to_numpy(), labels=self.valdf[\"label\"].to_numpy()) # TODO: evaluate your custom model in an equivalent way to accuracy_score.\n",
    "            rocauc = model.eval_rocauc(X=self.valdf[\"review\"].to_numpy(), labels=self.valdf[\"label\"].to_numpy()) # TODO: evaluate your custom model in an equivalent way to roc_auc_score.\n",
    "            self.results.append(ModelResult(f\"NbowModel - vocab_sz: {params['vocab_sz']}\", params, pathspec, acc, rocauc))\n",
    "\n",
    "        self.next(self.aggregate)\n",
    "\n",
    "    @step\n",
    "    def aggregate(self, inputs):\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BaselineChallenge()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Anticipating Failure in Your Machine Learning Project\n",
    "\n",
    "In this task, you'll practice anticipating potential failure modes in a sentiment analysis classifier and develop strategies to mitigate them. Here's what you'll need to do:\n",
    "### Step 1: Identify Potential Failure Modes\n",
    "\n",
    "The first step in anticipating failure in your machine learning project is to identify potential failure modes. Start by brainstorming ways in which your project could fail from an engineering point of view. For example, your model could overfit to the training data or suffer from data bias.\n",
    "\n",
    "1. As stated above, model can overfit on training data and might not generalize on new data.\n",
    "2. When the model is actually deployed, data distribution might change. For example:\n",
    "    1. Domain changes - Model trained on women's clothing review might not do as well on electronics. Or reviews on amazon vs reviews on twitter might look different.\n",
    "    2. Other kind of distribution change - training data had reviews from all age group but in production most reviews are from young people. This could be a problem because what if model doesn't perform as well for young people.\n",
    "3. Lack of diversity in training data. This is related to 2.1. Depending upon how the data was collected, model might be able to generalize for only a small subset of people.\n",
    "\n",
    "### Step 2: Develop Strategies to Mitigate Failure Modes\n",
    "\n",
    "Once you've identified potential failure modes, the next step is to develop strategies to mitigate them. Think about what measures you could take to fix the issue if it were to occur. For example, if your model is overfitting to the training data, you could try regularization techniques such as L1 or L2 regularization to reduce the complexity of the model.\n",
    "\n",
    "1. Model should be evaluated thoroughly before deployment to understand where it does well and where it doesn't.\n",
    "2. Model should be constantly monitored during deployment to understand if the data distribution has changed, or if the model is doing something strange.\n",
    "3. Model should be regularly updated with new data.\n",
    "4. Training data should be explored and understood well because it creates an upper bound on model's applicability.\n",
    "\n",
    "### Step 3: Plan Ahead to Avoid Failure Modes\n",
    "\n",
    "Finally, it's important to plan ahead to avoid potential failure modes in the first place. Think about what you could have done initially to avoid these failure modes. For example, you could have collected a diverse set of training data to reduce data bias or experimented with different model architectures to find the best solution for your problem.\n",
    "\n",
    "The key to anticipating failure in your machine learning project is to be proactive rather than reactive. By identifying potential failure modes ahead of time and developing strategies to mitigate them, you'll be better equipped to build a successful machine learning project.\n",
    "\n",
    "1. Understanding the requirement, i.e. where the model would be deployed, is important and can help in planning model development and data collection accordingly.\n",
    "2. Any model can fail, no matter how good it performs on training data. Deployment should be done keeping this in mind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Visualizing ML Results with MF Cards\n",
    "Now it is time to iterate. Extend the flow in your `baseline_challenge.py` file to include a step that aggregates all of the results from hyperparameter tuning jobs, and logs results and a data visualiation in a Metaflow card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting baseline_challenge.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile baseline_challenge.py\n",
    "# TODO: In this cell, write your BaselineChallenge flow in the baseline_challenge.py file.\n",
    "\n",
    "from metaflow import FlowSpec, step, Flow, current, Parameter, IncludeFile, card, current\n",
    "from metaflow.cards import Table, Markdown, Artifact, Image\n",
    "import numpy as np \n",
    "from dataclasses import dataclass\n",
    "\n",
    "label_cutoff = 4\n",
    "labeling_function = lambda row: 1 * (row[\"rating\"] > label_cutoff) # TODO: Define your labeling function here.\n",
    "\n",
    "@dataclass\n",
    "class ModelResult:\n",
    "    \"A custom struct for storing model evaluation results.\"\n",
    "    name: None\n",
    "    params: None\n",
    "    pathspec: None\n",
    "    acc: None\n",
    "    rocauc: None\n",
    "\n",
    "class BaselineChallenge(FlowSpec):\n",
    "\n",
    "    split_size = Parameter('split-sz', default=0.2)\n",
    "    data = IncludeFile('data', default='/home/workspace/workspaces/full-stack-ml-metaflow-corise-week-1/data/Womens Clothing E-Commerce Reviews.csv')\n",
    "    kfold = Parameter('k', default=5)\n",
    "    scoring = Parameter('scoring', default='accuracy')\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "\n",
    "        import pandas as pd\n",
    "        import io \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # load dataset packaged with the flow.\n",
    "        # this technique is convenient when working with small datasets that need to move to remove tasks.\n",
    "        df = pd.read_csv(io.StringIO(self.data))\n",
    "        # TODO: load the data. \n",
    "        # Look up a few lines to the IncludeFile('data', default='Womens Clothing E-Commerce Reviews.csv'). \n",
    "        # You can find documentation on IncludeFile here: https://docs.metaflow.org/scaling/data#data-in-local-files\n",
    "\n",
    "        # filter down to reviews and labels \n",
    "        df.columns = [\"_\".join(name.lower().strip().split()) for name in df.columns]\n",
    "        df = df[~df.review_text.isna()]\n",
    "        df['review'] = df['review_text'].astype('str')\n",
    "        _has_review_df = df[df['review_text'] != 'nan']\n",
    "        reviews = _has_review_df['review_text']\n",
    "        labels = _has_review_df.apply(labeling_function, axis=1)\n",
    "        self.df = pd.DataFrame({'label': labels, **_has_review_df})\n",
    "\n",
    "        # split the data 80/20, or by using the flow's split-sz CLI argument\n",
    "        _df = pd.DataFrame({'review': reviews, 'label': labels})\n",
    "        self.traindf, self.valdf = train_test_split(_df, test_size=self.split_size)\n",
    "        print(f'num of rows in train set: {self.traindf.shape[0]}')\n",
    "        print(f'num of rows in validation set: {self.valdf.shape[0]}')\n",
    "\n",
    "        self.next(self.baseline, self.model)\n",
    "\n",
    "\n",
    "    @step\n",
    "    def baseline(self):\n",
    "        \"Compute the baseline\"\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "        self._name = \"baseline\"\n",
    "        params = \"Always predict the majority class: \"\n",
    "        pathspec = f\"{current.flow_name}/{current.run_id}/{current.step_name}/{current.task_id}\"\n",
    "        \n",
    "        majority_class = self.traindf[\"label\"].mode()[0]\n",
    "        params += str(majority_class)\n",
    "\n",
    "        val_pred = [majority_class] * len(self.valdf)\n",
    "        val_score = [majority_class] * len(self.valdf)\n",
    "        predictions = val_pred # TODO: predict the majority class\n",
    "        acc = accuracy_score(self.valdf[\"label\"], val_pred) # TODO: return the accuracy_score of these predictions\n",
    "         \n",
    "        rocauc = roc_auc_score(self.valdf[\"label\"], val_score) # TODO: return the roc_auc_score of these predictions\n",
    "        self.result = ModelResult(\"Baseline\", params, pathspec, acc, rocauc)\n",
    "        self.next(self.aggregate)\n",
    "\n",
    "\n",
    "    @step\n",
    "    def model(self):\n",
    "\n",
    "        # TODO: import your model if it is defined in another file.\n",
    "        from model import NbowModel\n",
    "\n",
    "        self._name = \"model\"\n",
    "        # NOTE: If you followed the link above to find a custom model implementation, \n",
    "            # you will have noticed your model's vocab_sz hyperparameter.\n",
    "            # Too big of vocab_sz causes an error. Can you explain why? \n",
    "            # > Because of linear layer. Linear layer assumes `vocab_sz` number of input features. \n",
    "            # > If we set `vocab_sz` to a very large value, CountVectorizer doesn't see that many unique words, \n",
    "            # > and returns lesser number of features. \n",
    "            # > (Like linear layer expects 1 million features but CountVectorizer gives only 800 features!)\n",
    "        self.model_params = [{'vocab_sz': 100}, {'vocab_sz': 300}, {'vocab_sz': 500}, {'vocab_sz': 700}]\n",
    "        self.fit_params = [{\"batch_size\": 32, \"epochs\": 5}, {\"batch_size\": 32, \"epochs\": 10}, {\"batch_size\": 32, \"epochs\": 20}, \n",
    "                           {\"batch_size\": 256, \"epochs\": 5}, {\"batch_size\": 256, \"epochs\": 10}, {\"batch_size\": 256, \"epochs\": 20},\n",
    "                           {\"batch_size\": 512, \"epochs\": 5}, {\"batch_size\": 512, \"epochs\": 10}, {\"batch_size\": 512, \"epochs\": 20},]\n",
    "\n",
    "        pathspec = f\"{current.flow_name}/{current.run_id}/{current.step_name}/{current.task_id}\"\n",
    "\n",
    "        self.results = []\n",
    "        for model_params in self.model_params:\n",
    "            for fit_params in self.fit_params:\n",
    "                params = {**model_params, **fit_params}\n",
    "\n",
    "                model = NbowModel(**model_params) # TODO: instantiate your custom model here!\n",
    "                model.fit(X=self.traindf['review'].to_numpy(), y=self.traindf['label'].to_numpy(), **fit_params)\n",
    "                acc = model.eval_acc(X=self.valdf[\"review\"].to_numpy(), labels=self.valdf[\"label\"].to_numpy()) # TODO: evaluate your custom model in an equivalent way to accuracy_score.\n",
    "                rocauc = model.eval_rocauc(X=self.valdf[\"review\"].to_numpy(), labels=self.valdf[\"label\"].to_numpy()) # TODO: evaluate your custom model in an equivalent way to roc_auc_score.\n",
    "\n",
    "                self.results.append(ModelResult(f\"NbowModel (vocab, batch, epoch) - {list(params.values())}\", params, pathspec, acc, rocauc))\n",
    "\n",
    "        self.next(self.aggregate)\n",
    "\n",
    "\n",
    "    def add_one(self, rows, result, df):\n",
    "        \"A helper function to load results.\"\n",
    "        rows.append([\n",
    "            Markdown(result.name),\n",
    "            Artifact(result.params),\n",
    "            Artifact(result.pathspec),\n",
    "            Artifact(result.acc),\n",
    "            Artifact(result.rocauc)\n",
    "        ])\n",
    "        df['name'].append(result.name)\n",
    "        df['accuracy'].append(result.acc)\n",
    "        return rows, df\n",
    "\n",
    "    @card(type=\"corise\") # TODO: Set your card type to \"corise\". \n",
    "            # I wonder what other card types there are?\n",
    "            # https://docs.metaflow.org/metaflow/visualizing-results\n",
    "            # https://github.com/outerbounds/metaflow-card-altair/blob/main/altairflow.py\n",
    "    @step\n",
    "    def aggregate(self, inputs):\n",
    "        import pandas as pd\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        from matplotlib import rcParams \n",
    "        rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "        rows = []\n",
    "        plot_df = {'name': [], 'accuracy': []}\n",
    "        for task in inputs:\n",
    "            if task._name == \"model\": \n",
    "                for result in task.results:\n",
    "                    print(result)\n",
    "                    rows, plot_df = self.add_one(rows, result, plot_df)\n",
    "            elif task._name == \"baseline\":\n",
    "                print(task.result)\n",
    "                rows, plot_df = self.add_one(rows, task.result, plot_df)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown task._name type. Cannot parse results.\")\n",
    "            \n",
    "        current.card.append(Markdown(\"# All models from this flow run\"))\n",
    "\n",
    "        # TODO: Add a Table of the results to your card! \n",
    "        current.card.append(\n",
    "            Table(\n",
    "                rows, # TODO: What goes here to populate the Table in the card? \n",
    "                headers=[\"Model name\", \"Params\", \"Task pathspec\", \"Accuracy\", \"ROCAUC\"]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig, ax = plt.subplots(1,1, dpi=300)\n",
    "        plt.xticks(rotation=90)\n",
    "        sns.barplot(data=pd.DataFrame(plot_df), x=\"name\", y=\"accuracy\", ax=ax)\n",
    "        \n",
    "        # TODO: Append the matplotlib fig to the card\n",
    "        # Docs: https://docs.metaflow.org/metaflow/visualizing-results/easy-custom-reports-with-card-components#showing-plots\n",
    "        current.card.append(Image.from_matplotlib(fig))\n",
    "\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BaselineChallenge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.8.3.1+ob(v1)\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mBaselineChallenge\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:sandbox\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[22mIncluding file /home/workspace/workspaces/full-stack-ml-metaflow-corise-week-1/data/Womens Clothing E-Commerce Reviews.csv of size 8MB \u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[35m2023-05-14 18:35:48.623 \u001b[0m\u001b[1mWorkflow starting (run-id 26), see it in the UI at https://ui-pw-1410177943.outerbounds.dev/BaselineChallenge/26\u001b[0m\n",
      "\u001b[35m2023-05-14 18:35:48.889 \u001b[0m\u001b[32m[26/start/139 (pid 17312)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:35:52.122 \u001b[0m\u001b[32m[26/start/139 (pid 17312)] \u001b[0m\u001b[22mnum of rows in train set: 18112\u001b[0m\n",
      "\u001b[35m2023-05-14 18:35:54.893 \u001b[0m\u001b[32m[26/start/139 (pid 17312)] \u001b[0m\u001b[22mnum of rows in validation set: 4529\u001b[0m\n",
      "\u001b[35m2023-05-14 18:35:55.119 \u001b[0m\u001b[32m[26/start/139 (pid 17312)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:35:55.465 \u001b[0m\u001b[32m[26/baseline/140 (pid 17412)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:35:55.664 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:00.198 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m2023-05-14 18:36:00.198579: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:00.337 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:01.197 \u001b[0m\u001b[32m[26/baseline/140 (pid 17412)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:01.352 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:01.352 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:01.352 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m2023-05-14 18:36:01.055077: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:01.352 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/5\u001b[0m\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6422 - accuracy: 0.6327 - val_loss: 0.5876 - val_accuracy: 0.6980\u001b[0m.7293 - accuracy: 0.43\n",
      "\u001b[35m2023-05-14 18:36:02.278 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/5\u001b[0m\n",
      "453/453 [==============================] - 0s 811us/step - loss: 0.5787 - accuracy: 0.7071 - val_loss: 0.5786 - val_accuracy: 0.7060\u001b[0m8 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:02.646 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/5\u001b[0m\n",
      "453/453 [==============================] - 0s 847us/step - loss: 0.5654 - accuracy: 0.7183 - val_loss: 0.5757 - val_accuracy: 0.7080\u001b[0m4 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:36:03.031 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/5\u001b[0m\n",
      "453/453 [==============================] - 0s 824us/step - loss: 0.5678 - accuracy: 0.7125 - val_loss: 0.5759 - val_accuracy: 0.7096\u001b[0m3 - accuracy: 0.62\n",
      "\u001b[35m2023-05-14 18:36:03.406 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/5\u001b[0m\n",
      "453/453 [==============================] - 0s 810us/step - loss: 0.5559 - accuracy: 0.7192 - val_loss: 0.5753 - val_accuracy: 0.7033\u001b[0m5 - accuracy: 0.68\n",
      "\u001b[35m2023-05-14 18:36:03.830 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:04.086 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:04.087 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:04.309 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:04.309 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:04.981 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:04.981 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.6422 - accuracy: 0.6370 - val_loss: 0.5845 - val_accuracy: 0.7080\u001b[0m.7343 - accuracy: 0.56\n",
      "\u001b[35m2023-05-14 18:36:05.751 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "453/453 [==============================] - 0s 871us/step - loss: 0.5773 - accuracy: 0.7054 - val_loss: 0.5809 - val_accuracy: 0.7080\u001b[0m8 - accuracy: 0.62\n",
      "\u001b[35m2023-05-14 18:36:06.146 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "453/453 [==============================] - 0s 867us/step - loss: 0.5716 - accuracy: 0.7109 - val_loss: 0.5787 - val_accuracy: 0.7047\u001b[0m8 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:36:06.540 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "453/453 [==============================] - 0s 845us/step - loss: 0.5612 - accuracy: 0.7204 - val_loss: 0.5764 - val_accuracy: 0.7088\u001b[0m1 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:06.924 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "453/453 [==============================] - 0s 846us/step - loss: 0.5547 - accuracy: 0.7224 - val_loss: 0.5731 - val_accuracy: 0.7132\u001b[0m3 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:36:07.308 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "453/453 [==============================] - 0s 832us/step - loss: 0.5579 - accuracy: 0.7215 - val_loss: 0.5751 - val_accuracy: 0.7052\u001b[0m3 - accuracy: 0.68\n",
      "\u001b[35m2023-05-14 18:36:07.686 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "453/453 [==============================] - 0s 834us/step - loss: 0.5527 - accuracy: 0.7230 - val_loss: 0.5779 - val_accuracy: 0.7005\u001b[0m7 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:08.064 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "453/453 [==============================] - 0s 847us/step - loss: 0.5466 - accuracy: 0.7319 - val_loss: 0.5774 - val_accuracy: 0.6975\u001b[0m8 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:08.449 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "453/453 [==============================] - 0s 909us/step - loss: 0.5454 - accuracy: 0.7302 - val_loss: 0.5815 - val_accuracy: 0.7030\u001b[0m3 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:08.861 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "453/453 [==============================] - 0s 821us/step - loss: 0.5372 - accuracy: 0.7359 - val_loss: 0.5812 - val_accuracy: 0.7069\u001b[0m8 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:09.234 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:09.457 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:09.457 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:09.680 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:09.680 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:10.352 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:10.352 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.6615 - accuracy: 0.6172 - val_loss: 0.5818 - val_accuracy: 0.7077\u001b[0m.7762 - accuracy: 0.59\n",
      "\u001b[35m2023-05-14 18:36:11.095 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/20\u001b[0m\n",
      "453/453 [==============================] - 0s 879us/step - loss: 0.5721 - accuracy: 0.7106 - val_loss: 0.5765 - val_accuracy: 0.7069\u001b[0m9 - accuracy: 0.93\n",
      "\u001b[35m2023-05-14 18:36:11.495 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/20\u001b[0m\n",
      "453/453 [==============================] - 0s 857us/step - loss: 0.5658 - accuracy: 0.7171 - val_loss: 0.5753 - val_accuracy: 0.7091\u001b[0m0 - accuracy: 0.68\n",
      "\u001b[35m2023-05-14 18:36:11.884 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/20\u001b[0m\n",
      "453/453 [==============================] - 0s 830us/step - loss: 0.5591 - accuracy: 0.7245 - val_loss: 0.5745 - val_accuracy: 0.7044\u001b[0m4 - accuracy: 0.68\n",
      "\u001b[35m2023-05-14 18:36:12.261 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/20\u001b[0m\n",
      "453/453 [==============================] - 0s 856us/step - loss: 0.5586 - accuracy: 0.7187 - val_loss: 0.5724 - val_accuracy: 0.7060\u001b[0m4 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:12.650 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/20\u001b[0m\n",
      "453/453 [==============================] - 0s 863us/step - loss: 0.5630 - accuracy: 0.7146 - val_loss: 0.5748 - val_accuracy: 0.7071\u001b[0m5 - accuracy: 0.62\n",
      "\u001b[35m2023-05-14 18:36:13.042 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/20\u001b[0m\n",
      "453/453 [==============================] - 0s 849us/step - loss: 0.5524 - accuracy: 0.7233 - val_loss: 0.5779 - val_accuracy: 0.7060\u001b[0m0 - accuracy: 0.65\n",
      "\u001b[35m2023-05-14 18:36:13.427 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/20\u001b[0m\n",
      "453/453 [==============================] - 0s 835us/step - loss: 0.5512 - accuracy: 0.7279 - val_loss: 0.5718 - val_accuracy: 0.7110\u001b[0m0 - accuracy: 0.65\n",
      "\u001b[35m2023-05-14 18:36:13.807 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/20\u001b[0m\n",
      "453/453 [==============================] - 0s 813us/step - loss: 0.5424 - accuracy: 0.7351 - val_loss: 0.5742 - val_accuracy: 0.7154\u001b[0m6 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:14.176 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/20\u001b[0m\n",
      "453/453 [==============================] - 0s 829us/step - loss: 0.5392 - accuracy: 0.7341 - val_loss: 0.5793 - val_accuracy: 0.7121\u001b[0m3 - accuracy: 0.65\n",
      "\u001b[35m2023-05-14 18:36:14.553 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 11/20\u001b[0m\n",
      "453/453 [==============================] - 0s 846us/step - loss: 0.5413 - accuracy: 0.7349 - val_loss: 0.5769 - val_accuracy: 0.7063\u001b[0m0 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:14.937 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 12/20\u001b[0m\n",
      "453/453 [==============================] - 0s 835us/step - loss: 0.5417 - accuracy: 0.7307 - val_loss: 0.5771 - val_accuracy: 0.7124\u001b[0m3 - accuracy: 0.68\n",
      "\u001b[35m2023-05-14 18:36:15.316 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 13/20\u001b[0m\n",
      "453/453 [==============================] - 0s 842us/step - loss: 0.5397 - accuracy: 0.7320 - val_loss: 0.5822 - val_accuracy: 0.7107\u001b[0m5 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:15.699 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 14/20\u001b[0m\n",
      "453/453 [==============================] - 0s 860us/step - loss: 0.5301 - accuracy: 0.7436 - val_loss: 0.5804 - val_accuracy: 0.7099\u001b[0m5 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:36:16.089 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 15/20\u001b[0m\n",
      "453/453 [==============================] - 0s 836us/step - loss: 0.5390 - accuracy: 0.7359 - val_loss: 0.5862 - val_accuracy: 0.7025\u001b[0m8 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:16.469 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 16/20\u001b[0m\n",
      "453/453 [==============================] - 0s 822us/step - loss: 0.5333 - accuracy: 0.7425 - val_loss: 0.5867 - val_accuracy: 0.7049\u001b[0m4 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:16.843 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 17/20\u001b[0m\n",
      "453/453 [==============================] - 0s 863us/step - loss: 0.5294 - accuracy: 0.7409 - val_loss: 0.5886 - val_accuracy: 0.7055\u001b[0m8 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:17.234 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 18/20\u001b[0m\n",
      "453/453 [==============================] - 0s 846us/step - loss: 0.5315 - accuracy: 0.7409 - val_loss: 0.5903 - val_accuracy: 0.7002\u001b[0m0 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:17.619 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 19/20\u001b[0m\n",
      "453/453 [==============================] - 0s 833us/step - loss: 0.5280 - accuracy: 0.7449 - val_loss: 0.5910 - val_accuracy: 0.7060\u001b[0m6 - accuracy: 0.68\n",
      "\u001b[35m2023-05-14 18:36:17.997 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 20/20\u001b[0m\n",
      "453/453 [==============================] - 0s 838us/step - loss: 0.5236 - accuracy: 0.7461 - val_loss: 0.5913 - val_accuracy: 0.7044\u001b[0m7 - accuracy: 0.93\n",
      "\u001b[35m2023-05-14 18:36:18.378 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:18.600 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:18.600 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:18.861 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:18.862 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:19.499 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:19.499 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/5\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.6980 - accuracy: 0.5386 - val_loss: 0.6130 - val_accuracy: 0.6842\u001b[0m.7717 - accuracy: 0.398\n",
      "\u001b[35m2023-05-14 18:36:19.950 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/5\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.6954 - val_loss: 0.5856 - val_accuracy: 0.7083\u001b[0m6403 - accuracy: 0.66\n",
      "\u001b[35m2023-05-14 18:36:20.039 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/5\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5745 - accuracy: 0.7139 - val_loss: 0.5806 - val_accuracy: 0.7099\u001b[0m5474 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:36:20.120 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/5\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5688 - accuracy: 0.7166 - val_loss: 0.5801 - val_accuracy: 0.7107\u001b[0m6093 - accuracy: 0.69\n",
      "\u001b[35m2023-05-14 18:36:20.201 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/5\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5631 - accuracy: 0.7191 - val_loss: 0.5787 - val_accuracy: 0.7083\u001b[0m5770 - accuracy: 0.70\n",
      "\u001b[35m2023-05-14 18:36:20.284 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:20.511 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:20.511 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:20.742 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:20.742 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:21.390 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:21.390 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.6815 - accuracy: 0.5916 - val_loss: 0.6210 - val_accuracy: 0.6760\u001b[0m.7509 - accuracy: 0.535\n",
      "\u001b[35m2023-05-14 18:36:21.832 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6034 - accuracy: 0.6903 - val_loss: 0.5875 - val_accuracy: 0.7027\u001b[0m6302 - accuracy: 0.64\n",
      "\u001b[35m2023-05-14 18:36:21.919 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5685 - accuracy: 0.7157 - val_loss: 0.5822 - val_accuracy: 0.7069\u001b[0m5311 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:36:22.003 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5713 - accuracy: 0.7124 - val_loss: 0.5801 - val_accuracy: 0.7094\u001b[0m5866 - accuracy: 0.66\n",
      "\u001b[35m2023-05-14 18:36:22.083 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5634 - accuracy: 0.7170 - val_loss: 0.5791 - val_accuracy: 0.7077\u001b[0m5381 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:22.163 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5645 - accuracy: 0.7185 - val_loss: 0.5777 - val_accuracy: 0.7063\u001b[0m5436 - accuracy: 0.74\n",
      "\u001b[35m2023-05-14 18:36:22.249 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5573 - accuracy: 0.7257 - val_loss: 0.5770 - val_accuracy: 0.7077\u001b[0m5294 - accuracy: 0.74\n",
      "\u001b[35m2023-05-14 18:36:22.341 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5674 - accuracy: 0.7140 - val_loss: 0.5756 - val_accuracy: 0.7091\u001b[0m5881 - accuracy: 0.69\n",
      "\u001b[35m2023-05-14 18:36:22.422 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5600 - accuracy: 0.7219 - val_loss: 0.5755 - val_accuracy: 0.7066\u001b[0m5338 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:22.503 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5554 - accuracy: 0.7216 - val_loss: 0.5753 - val_accuracy: 0.7132\u001b[0m5397 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:36:22.584 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:22.818 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:22.818 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:23.043 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:23.043 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:23.710 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:23.710 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/20\u001b[0m\n",
      "57/57 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5576 - val_loss: 0.6189 - val_accuracy: 0.6793\u001b[0m.7217 - accuracy: 0.515\n",
      "\u001b[35m2023-05-14 18:36:24.337 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5989 - accuracy: 0.7011 - val_loss: 0.5865 - val_accuracy: 0.6997\u001b[0m5907 - accuracy: 0.69\n",
      "\u001b[35m2023-05-14 18:36:24.416 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5796 - accuracy: 0.7092 - val_loss: 0.5802 - val_accuracy: 0.7060\u001b[0m6036 - accuracy: 0.69\n",
      "\u001b[35m2023-05-14 18:36:24.495 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5682 - accuracy: 0.7184 - val_loss: 0.5802 - val_accuracy: 0.7110\u001b[0m5900 - accuracy: 0.69\n",
      "\u001b[35m2023-05-14 18:36:24.576 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5669 - accuracy: 0.7165 - val_loss: 0.5779 - val_accuracy: 0.7096\u001b[0m5946 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:24.656 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5620 - accuracy: 0.7188 - val_loss: 0.5769 - val_accuracy: 0.7143\u001b[0m5290 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:36:24.734 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5619 - accuracy: 0.7184 - val_loss: 0.5773 - val_accuracy: 0.7129\u001b[0m5680 - accuracy: 0.70\n",
      "\u001b[35m2023-05-14 18:36:24.813 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5555 - accuracy: 0.7215 - val_loss: 0.5752 - val_accuracy: 0.7127\u001b[0m5504 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:24.894 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5606 - accuracy: 0.7184 - val_loss: 0.5754 - val_accuracy: 0.7138\u001b[0m5519 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:36:24.974 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5653 - accuracy: 0.7182 - val_loss: 0.5742 - val_accuracy: 0.7152\u001b[0m5575 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:36:25.052 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 11/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5560 - accuracy: 0.7242 - val_loss: 0.5740 - val_accuracy: 0.7160\u001b[0m5624 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:25.131 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 12/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5570 - accuracy: 0.7225 - val_loss: 0.5728 - val_accuracy: 0.7154\u001b[0m5891 - accuracy: 0.72\n",
      "\u001b[35m2023-05-14 18:36:25.214 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 13/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5557 - accuracy: 0.7242 - val_loss: 0.5723 - val_accuracy: 0.7135\u001b[0m5517 - accuracy: 0.72\n",
      "\u001b[35m2023-05-14 18:36:25.293 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 14/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5463 - accuracy: 0.7325 - val_loss: 0.5733 - val_accuracy: 0.7127\u001b[0m5729 - accuracy: 0.70\n",
      "\u001b[35m2023-05-14 18:36:25.380 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 15/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5497 - accuracy: 0.7271 - val_loss: 0.5731 - val_accuracy: 0.7118\u001b[0m5450 - accuracy: 0.70\n",
      "\u001b[35m2023-05-14 18:36:25.509 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 16/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5536 - accuracy: 0.7221 - val_loss: 0.5748 - val_accuracy: 0.7055\u001b[0m5613 - accuracy: 0.69\n",
      "\u001b[35m2023-05-14 18:36:25.608 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 17/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5493 - accuracy: 0.7235 - val_loss: 0.5728 - val_accuracy: 0.7118\u001b[0m5815 - accuracy: 0.69\n",
      "\u001b[35m2023-05-14 18:36:25.690 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 18/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5423 - accuracy: 0.7318 - val_loss: 0.5736 - val_accuracy: 0.7091\u001b[0m5580 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:25.773 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 19/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5405 - accuracy: 0.7375 - val_loss: 0.5734 - val_accuracy: 0.7135\u001b[0m5417 - accuracy: 0.76\n",
      "\u001b[35m2023-05-14 18:36:25.859 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 20/20\u001b[0m\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.5444 - accuracy: 0.7274 - val_loss: 0.5739 - val_accuracy: 0.7088\u001b[0m5233 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:25.941 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:26.169 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:26.169 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:26.412 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:26.413 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:27.108 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:27.108 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/5\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.6744 - accuracy: 0.5843 - val_loss: 0.6245 - val_accuracy: 0.6715\u001b[0m6867 - accuracy: 0.56\n",
      "\u001b[35m2023-05-14 18:36:27.519 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/5\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.6111 - accuracy: 0.6859 - val_loss: 0.5920 - val_accuracy: 0.7014\u001b[0m6177 - accuracy: 0.67\n",
      "\u001b[35m2023-05-14 18:36:27.578 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/5\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5822 - accuracy: 0.7092 - val_loss: 0.5838 - val_accuracy: 0.7077\u001b[0m5948 - accuracy: 0.68\n",
      "\u001b[35m2023-05-14 18:36:27.636 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/5\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5816 - accuracy: 0.7032 - val_loss: 0.5827 - val_accuracy: 0.7088\u001b[0m6131 - accuracy: 0.68\n",
      "\u001b[35m2023-05-14 18:36:27.699 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/5\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7190 - val_loss: 0.5807 - val_accuracy: 0.7083\u001b[0m5399 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:36:27.755 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:27.981 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:27.981 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:28.207 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:28.208 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:28.875 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:28.875 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.6876 - accuracy: 0.5634 - val_loss: 0.6435 - val_accuracy: 0.6519\u001b[0m7168 - accuracy: 0.49\n",
      "\u001b[35m2023-05-14 18:36:29.278 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.6169 - accuracy: 0.6825 - val_loss: 0.6048 - val_accuracy: 0.6895\u001b[0m6141 - accuracy: 0.70\n",
      "\u001b[35m2023-05-14 18:36:29.335 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5868 - accuracy: 0.7021 - val_loss: 0.5932 - val_accuracy: 0.6969\u001b[0m5935 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:29.403 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.7105 - val_loss: 0.5900 - val_accuracy: 0.7025\u001b[0m5804 - accuracy: 0.72\n",
      "\u001b[35m2023-05-14 18:36:29.470 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5688 - accuracy: 0.7177 - val_loss: 0.5859 - val_accuracy: 0.7077\u001b[0m5482 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:36:29.526 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5711 - accuracy: 0.7132 - val_loss: 0.5866 - val_accuracy: 0.7055\u001b[0m5739 - accuracy: 0.70\n",
      "\u001b[35m2023-05-14 18:36:29.591 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5662 - accuracy: 0.7167 - val_loss: 0.5854 - val_accuracy: 0.7074\u001b[0m5554 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:29.647 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5668 - accuracy: 0.7205 - val_loss: 0.5842 - val_accuracy: 0.7107\u001b[0m5588 - accuracy: 0.72\n",
      "\u001b[35m2023-05-14 18:36:29.708 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5609 - accuracy: 0.7205 - val_loss: 0.5835 - val_accuracy: 0.7085\u001b[0m5507 - accuracy: 0.70\n",
      "\u001b[35m2023-05-14 18:36:29.769 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5584 - accuracy: 0.7247 - val_loss: 0.5819 - val_accuracy: 0.7107\u001b[0m5705 - accuracy: 0.74\n",
      "\u001b[35m2023-05-14 18:36:29.827 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:30.045 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:30.045 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:30.314 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:30.314 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:30.980 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:30.980 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/20\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.7064 - accuracy: 0.5624 - val_loss: 0.6469 - val_accuracy: 0.6404\u001b[0m7500 - accuracy: 0.52\n",
      "\u001b[35m2023-05-14 18:36:31.393 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.6340 - accuracy: 0.6545 - val_loss: 0.6060 - val_accuracy: 0.6909\u001b[0m6535 - accuracy: 0.63\n",
      "\u001b[35m2023-05-14 18:36:31.451 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5949 - accuracy: 0.6886 - val_loss: 0.5884 - val_accuracy: 0.7016\u001b[0m5886 - accuracy: 0.68\n",
      "\u001b[35m2023-05-14 18:36:31.511 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5796 - accuracy: 0.7048 - val_loss: 0.5832 - val_accuracy: 0.7094\u001b[0m5823 - accuracy: 0.69\n",
      "\u001b[35m2023-05-14 18:36:31.582 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5742 - accuracy: 0.7139 - val_loss: 0.5823 - val_accuracy: 0.7129\u001b[0m5717 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:31.637 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5815 - accuracy: 0.7038 - val_loss: 0.5827 - val_accuracy: 0.7102\u001b[0m5940 - accuracy: 0.70\n",
      "\u001b[35m2023-05-14 18:36:31.696 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5744 - accuracy: 0.7127 - val_loss: 0.5815 - val_accuracy: 0.7105\u001b[0m5961 - accuracy: 0.70\n",
      "\u001b[35m2023-05-14 18:36:31.752 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5707 - accuracy: 0.7146 - val_loss: 0.5796 - val_accuracy: 0.7107\u001b[0m5513 - accuracy: 0.69\n",
      "\u001b[35m2023-05-14 18:36:31.813 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5668 - accuracy: 0.7175 - val_loss: 0.5798 - val_accuracy: 0.7099\u001b[0m5793 - accuracy: 0.69\n",
      "\u001b[35m2023-05-14 18:36:31.877 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5665 - accuracy: 0.7149 - val_loss: 0.5785 - val_accuracy: 0.7113\u001b[0m5860 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:31.931 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 11/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5622 - accuracy: 0.7178 - val_loss: 0.5779 - val_accuracy: 0.7110\u001b[0m5850 - accuracy: 0.70\n",
      "\u001b[35m2023-05-14 18:36:31.994 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 12/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7170 - val_loss: 0.5780 - val_accuracy: 0.7088\u001b[0m5541 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:32.049 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 13/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.7232 - val_loss: 0.5778 - val_accuracy: 0.7088\u001b[0m5468 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:32.109 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 14/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5590 - accuracy: 0.7225 - val_loss: 0.5784 - val_accuracy: 0.7083\u001b[0m5856 - accuracy: 0.70\n",
      "\u001b[35m2023-05-14 18:36:32.174 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 15/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5566 - accuracy: 0.7240 - val_loss: 0.5780 - val_accuracy: 0.7080\u001b[0m5446 - accuracy: 0.74\n",
      "\u001b[35m2023-05-14 18:36:32.230 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 16/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.7244 - val_loss: 0.5775 - val_accuracy: 0.7071\u001b[0m5447 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:32.289 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 17/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7140 - val_loss: 0.5767 - val_accuracy: 0.7066\u001b[0m5458 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:32.344 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 18/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7158 - val_loss: 0.5763 - val_accuracy: 0.7080\u001b[0m5441 - accuracy: 0.72\n",
      "\u001b[35m2023-05-14 18:36:32.406 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 19/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5544 - accuracy: 0.7241 - val_loss: 0.5758 - val_accuracy: 0.7069\u001b[0m5614 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:32.462 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 20/20\u001b[0m\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5517 - accuracy: 0.7254 - val_loss: 0.5760 - val_accuracy: 0.7085\u001b[0m5409 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:36:32.517 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:32.739 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:32.739 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:32.979 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:32.979 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:33.662 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:33.662 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/5\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.6027 - accuracy: 0.6769 - val_loss: 0.5310 - val_accuracy: 0.7543\u001b[0m.7110 - accuracy: 0.59\n",
      "\u001b[35m2023-05-14 18:36:34.464 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/5\u001b[0m\n",
      "453/453 [==============================] - 0s 984us/step - loss: 0.5145 - accuracy: 0.7622 - val_loss: 0.5256 - val_accuracy: 0.7486\u001b[0m7 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:36:34.911 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/5\u001b[0m\n",
      "453/453 [==============================] - 0s 987us/step - loss: 0.5052 - accuracy: 0.7656 - val_loss: 0.5195 - val_accuracy: 0.7579\u001b[0m6 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:36:35.359 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/5\u001b[0m\n",
      "453/453 [==============================] - 0s 988us/step - loss: 0.4889 - accuracy: 0.7711 - val_loss: 0.5168 - val_accuracy: 0.7546\u001b[0m6 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:35.807 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/5\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4905 - accuracy: 0.7719 - val_loss: 0.5176 - val_accuracy: 0.7516\u001b[0m470 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:36.269 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:36.536 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:36.536 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:36.779 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:36.779 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:37.490 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:37.490 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.6047 - accuracy: 0.6758 - val_loss: 0.5286 - val_accuracy: 0.7491\u001b[0m.7247 - accuracy: 0.53\n",
      "\u001b[35m2023-05-14 18:36:38.286 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "453/453 [==============================] - 0s 932us/step - loss: 0.5042 - accuracy: 0.7658 - val_loss: 0.5207 - val_accuracy: 0.7510\u001b[0m2 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:38.709 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "453/453 [==============================] - 0s 941us/step - loss: 0.4940 - accuracy: 0.7695 - val_loss: 0.5173 - val_accuracy: 0.7535\u001b[0m7 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:39.136 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "453/453 [==============================] - 0s 937us/step - loss: 0.4866 - accuracy: 0.7746 - val_loss: 0.5172 - val_accuracy: 0.7557\u001b[0m5 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:36:39.562 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "453/453 [==============================] - 0s 957us/step - loss: 0.4763 - accuracy: 0.7857 - val_loss: 0.5167 - val_accuracy: 0.7546\u001b[0m8 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:39.996 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "453/453 [==============================] - 0s 973us/step - loss: 0.4688 - accuracy: 0.7845 - val_loss: 0.5197 - val_accuracy: 0.7546\u001b[0m7 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:36:40.438 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "453/453 [==============================] - 0s 929us/step - loss: 0.4695 - accuracy: 0.7868 - val_loss: 0.5272 - val_accuracy: 0.7444\u001b[0m1 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:36:40.860 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "453/453 [==============================] - 0s 964us/step - loss: 0.4691 - accuracy: 0.7875 - val_loss: 0.5258 - val_accuracy: 0.7494\u001b[0m7 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:41.298 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "453/453 [==============================] - 0s 979us/step - loss: 0.4449 - accuracy: 0.8068 - val_loss: 0.5302 - val_accuracy: 0.7419\u001b[0m2 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:36:41.743 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "453/453 [==============================] - 0s 931us/step - loss: 0.4501 - accuracy: 0.7969 - val_loss: 0.5348 - val_accuracy: 0.7452\u001b[0m2 - accuracy: 0.68\n",
      "\u001b[35m2023-05-14 18:36:42.170 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:42.460 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:42.460 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:42.698 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:42.698 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:43.421 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:43.422 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.6023 - accuracy: 0.6813 - val_loss: 0.5268 - val_accuracy: 0.7532\u001b[0m.7009 - accuracy: 0.56\n",
      "\u001b[35m2023-05-14 18:36:44.210 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/20\u001b[0m\n",
      "453/453 [==============================] - 0s 936us/step - loss: 0.5021 - accuracy: 0.7659 - val_loss: 0.5161 - val_accuracy: 0.7502\u001b[0m8 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:44.635 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/20\u001b[0m\n",
      "453/453 [==============================] - 0s 932us/step - loss: 0.4887 - accuracy: 0.7738 - val_loss: 0.5144 - val_accuracy: 0.7579\u001b[0m1 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:36:45.058 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/20\u001b[0m\n",
      "453/453 [==============================] - 0s 944us/step - loss: 0.4788 - accuracy: 0.7820 - val_loss: 0.5130 - val_accuracy: 0.7560\u001b[0m6 - accuracy: 0.68\n",
      "\u001b[35m2023-05-14 18:36:45.486 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/20\u001b[0m\n",
      "453/453 [==============================] - 0s 978us/step - loss: 0.4810 - accuracy: 0.7767 - val_loss: 0.5166 - val_accuracy: 0.7590\u001b[0m4 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:45.930 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/20\u001b[0m\n",
      "453/453 [==============================] - 0s 965us/step - loss: 0.4703 - accuracy: 0.7848 - val_loss: 0.5184 - val_accuracy: 0.7555\u001b[0m2 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:36:46.369 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/20\u001b[0m\n",
      "453/453 [==============================] - 0s 947us/step - loss: 0.4764 - accuracy: 0.7852 - val_loss: 0.5230 - val_accuracy: 0.7563\u001b[0m0 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:46.799 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/20\u001b[0m\n",
      "453/453 [==============================] - 0s 974us/step - loss: 0.4596 - accuracy: 0.7936 - val_loss: 0.5217 - val_accuracy: 0.7519\u001b[0m7 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:47.241 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/20\u001b[0m\n",
      "453/453 [==============================] - 0s 959us/step - loss: 0.4522 - accuracy: 0.7937 - val_loss: 0.5248 - val_accuracy: 0.7483\u001b[0m5 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:47.676 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/20\u001b[0m\n",
      "453/453 [==============================] - 0s 974us/step - loss: 0.4471 - accuracy: 0.7996 - val_loss: 0.5322 - val_accuracy: 0.7499\u001b[0m1 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:36:48.119 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 11/20\u001b[0m\n",
      "453/453 [==============================] - 0s 965us/step - loss: 0.4359 - accuracy: 0.8053 - val_loss: 0.5388 - val_accuracy: 0.7439\u001b[0m6 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:36:48.557 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 12/20\u001b[0m\n",
      "453/453 [==============================] - 0s 960us/step - loss: 0.4322 - accuracy: 0.8120 - val_loss: 0.5435 - val_accuracy: 0.7425\u001b[0m5 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:36:48.993 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 13/20\u001b[0m\n",
      "453/453 [==============================] - 0s 953us/step - loss: 0.4276 - accuracy: 0.8091 - val_loss: 0.5551 - val_accuracy: 0.7356\u001b[0m7 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:36:49.426 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 14/20\u001b[0m\n",
      "453/453 [==============================] - 0s 957us/step - loss: 0.4236 - accuracy: 0.8178 - val_loss: 0.5555 - val_accuracy: 0.7383\u001b[0m2 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:36:49.861 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 15/20\u001b[0m\n",
      "453/453 [==============================] - 0s 976us/step - loss: 0.4266 - accuracy: 0.8082 - val_loss: 0.5619 - val_accuracy: 0.7386\u001b[0m6 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:50.304 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 16/20\u001b[0m\n",
      "453/453 [==============================] - 0s 971us/step - loss: 0.4098 - accuracy: 0.8237 - val_loss: 0.5647 - val_accuracy: 0.7372\u001b[0m0 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:50.745 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 17/20\u001b[0m\n",
      "453/453 [==============================] - 0s 993us/step - loss: 0.4156 - accuracy: 0.8242 - val_loss: 0.5755 - val_accuracy: 0.7353\u001b[0m7 - accuracy: 0.96\n",
      "\u001b[35m2023-05-14 18:36:51.195 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 18/20\u001b[0m\n",
      "453/453 [==============================] - 0s 968us/step - loss: 0.4097 - accuracy: 0.8243 - val_loss: 0.5737 - val_accuracy: 0.7336\u001b[0m2 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:36:51.635 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 19/20\u001b[0m\n",
      "453/453 [==============================] - 0s 968us/step - loss: 0.4044 - accuracy: 0.8284 - val_loss: 0.5888 - val_accuracy: 0.7350\u001b[0m5 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:36:52.074 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 20/20\u001b[0m\n",
      "453/453 [==============================] - 0s 948us/step - loss: 0.4039 - accuracy: 0.8310 - val_loss: 0.5861 - val_accuracy: 0.7328\u001b[0m3 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:36:52.508 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:52.759 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:52.759 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:53.012 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:53.012 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:53.697 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:53.697 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/5\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6552 - accuracy: 0.6188 - val_loss: 0.5527 - val_accuracy: 0.7414\u001b[0m.7089 - accuracy: 0.519\n",
      "\u001b[35m2023-05-14 18:36:54.192 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/5\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5275 - accuracy: 0.7544 - val_loss: 0.5262 - val_accuracy: 0.7541\u001b[0m5040 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:36:54.317 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/5\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7634 - val_loss: 0.5223 - val_accuracy: 0.7524\u001b[0m4672 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:36:54.442 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/5\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.5088 - accuracy: 0.7636 - val_loss: 0.5179 - val_accuracy: 0.7535\u001b[0m5711 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:36:54.590 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/5\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4936 - accuracy: 0.7731 - val_loss: 0.5169 - val_accuracy: 0.7538\u001b[0m4887 - accuracy: 0.76\n",
      "\u001b[35m2023-05-14 18:36:54.716 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:54.959 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:54.959 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:55.208 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:55.209 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:55.898 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:55.898 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "57/57 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6168 - val_loss: 0.5509 - val_accuracy: 0.7394\u001b[0m.7209 - accuracy: 0.472\n",
      "\u001b[35m2023-05-14 18:36:56.412 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.7554 - val_loss: 0.5253 - val_accuracy: 0.7538\u001b[0m5348 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:36:56.522 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5007 - accuracy: 0.7665 - val_loss: 0.5196 - val_accuracy: 0.7555\u001b[0m5000 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:36:56.664 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.7739 - val_loss: 0.5156 - val_accuracy: 0.7549\u001b[0m4776 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:56.801 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.7735 - val_loss: 0.5138 - val_accuracy: 0.7585\u001b[0m4621 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:36:56.999 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.7743 - val_loss: 0.5144 - val_accuracy: 0.7566\u001b[0m4912 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:57.125 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4762 - accuracy: 0.7851 - val_loss: 0.5128 - val_accuracy: 0.7552\u001b[0m4245 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:36:57.248 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4736 - accuracy: 0.7810 - val_loss: 0.5130 - val_accuracy: 0.7604\u001b[0m4819 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:36:57.391 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4698 - accuracy: 0.7841 - val_loss: 0.5111 - val_accuracy: 0.7577\u001b[0m4975 - accuracy: 0.76\n",
      "\u001b[35m2023-05-14 18:36:57.512 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.7832 - val_loss: 0.5125 - val_accuracy: 0.7579\u001b[0m4521 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:36:57.637 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:57.868 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:57.868 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:58.110 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:58.110 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:58.813 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:36:58.814 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/20\u001b[0m\n",
      "57/57 [==============================] - 1s 4ms/step - loss: 0.6780 - accuracy: 0.5808 - val_loss: 0.5605 - val_accuracy: 0.7334\u001b[0m.7332 - accuracy: 0.500\n",
      "\u001b[35m2023-05-14 18:36:59.536 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.7547 - val_loss: 0.5254 - val_accuracy: 0.7541\u001b[0m5691 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:36:59.644 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.5086 - accuracy: 0.7628 - val_loss: 0.5195 - val_accuracy: 0.7521\u001b[0m5169 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:59.791 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.7771 - val_loss: 0.5171 - val_accuracy: 0.7510\u001b[0m5072 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:36:59.916 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.7741 - val_loss: 0.5166 - val_accuracy: 0.7543\u001b[0m4875 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:00.034 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.7755 - val_loss: 0.5134 - val_accuracy: 0.7566\u001b[0m4563 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:00.176 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4745 - accuracy: 0.7825 - val_loss: 0.5137 - val_accuracy: 0.7524\u001b[0m5051 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:00.309 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.7851 - val_loss: 0.5125 - val_accuracy: 0.7577\u001b[0m4637 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:00.427 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4697 - accuracy: 0.7831 - val_loss: 0.5135 - val_accuracy: 0.7552\u001b[0m4344 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:00.575 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.7844 - val_loss: 0.5120 - val_accuracy: 0.7557\u001b[0m4399 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:00.704 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 11/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7874 - val_loss: 0.5120 - val_accuracy: 0.7582\u001b[0m4493 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:00.823 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 12/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4529 - accuracy: 0.7953 - val_loss: 0.5133 - val_accuracy: 0.7566\u001b[0m4402 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:37:00.944 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 13/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4574 - accuracy: 0.7950 - val_loss: 0.5138 - val_accuracy: 0.7563\u001b[0m4675 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:01.093 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 14/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4553 - accuracy: 0.7947 - val_loss: 0.5144 - val_accuracy: 0.7582\u001b[0m5212 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:37:01.216 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 15/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4374 - accuracy: 0.8099 - val_loss: 0.5147 - val_accuracy: 0.7579\u001b[0m4248 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:37:01.359 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 16/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4488 - accuracy: 0.8000 - val_loss: 0.5173 - val_accuracy: 0.7577\u001b[0m4152 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:01.495 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 17/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.7983 - val_loss: 0.5180 - val_accuracy: 0.7557\u001b[0m4363 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:01.615 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 18/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.8037 - val_loss: 0.5194 - val_accuracy: 0.7599\u001b[0m4339 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:01.740 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 19/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8106 - val_loss: 0.5234 - val_accuracy: 0.7519\u001b[0m4268 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:01.871 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 20/20\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.4247 - accuracy: 0.8142 - val_loss: 0.5218 - val_accuracy: 0.7549\u001b[0m4147 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:37:02.001 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:02.239 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:02.239 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:02.478 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:02.478 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:03.203 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:03.203 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/5\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.6683 - accuracy: 0.6004 - val_loss: 0.5979 - val_accuracy: 0.7088\u001b[0m7023 - accuracy: 0.54\n",
      "\u001b[35m2023-05-14 18:37:03.664 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/5\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5657 - accuracy: 0.7371 - val_loss: 0.5488 - val_accuracy: 0.7403\u001b[0m5787 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:37:03.772 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/5\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5226 - accuracy: 0.7578 - val_loss: 0.5331 - val_accuracy: 0.7474\u001b[0m5262 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:37:03.880 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/5\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5062 - accuracy: 0.7670 - val_loss: 0.5271 - val_accuracy: 0.7521\u001b[0m4896 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:03.986 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/5\u001b[0m\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7652 - val_loss: 0.5236 - val_accuracy: 0.7530\u001b[0m4658 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:04.086 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:04.325 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:04.325 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:04.612 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:04.612 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:05.299 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:05.299 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.6644 - accuracy: 0.6171 - val_loss: 0.5845 - val_accuracy: 0.7174\u001b[0m7017 - accuracy: 0.52\n",
      "\u001b[35m2023-05-14 18:37:05.764 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5550 - accuracy: 0.7451 - val_loss: 0.5361 - val_accuracy: 0.7430\u001b[0m5705 - accuracy: 0.74\n",
      "\u001b[35m2023-05-14 18:37:05.873 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5210 - accuracy: 0.7585 - val_loss: 0.5257 - val_accuracy: 0.7499\u001b[0m5326 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:37:05.985 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5096 - accuracy: 0.7646 - val_loss: 0.5224 - val_accuracy: 0.7508\u001b[0m5428 - accuracy: 0.74\n",
      "\u001b[35m2023-05-14 18:37:06.107 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.4970 - accuracy: 0.7716 - val_loss: 0.5237 - val_accuracy: 0.7510\u001b[0m4630 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:06.239 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4965 - accuracy: 0.7725 - val_loss: 0.5182 - val_accuracy: 0.7508\u001b[0m4802 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:06.368 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4917 - accuracy: 0.7707 - val_loss: 0.5189 - val_accuracy: 0.7472\u001b[0m4849 - accuracy: 0.76\n",
      "\u001b[35m2023-05-14 18:37:06.471 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4853 - accuracy: 0.7729 - val_loss: 0.5167 - val_accuracy: 0.7494\u001b[0m4856 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:37:06.572 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4835 - accuracy: 0.7733 - val_loss: 0.5167 - val_accuracy: 0.7474\u001b[0m4973 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:37:06.680 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4842 - accuracy: 0.7752 - val_loss: 0.5158 - val_accuracy: 0.7461\u001b[0m4933 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:06.787 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:07.036 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:07.036 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:07.270 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:07.270 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:07.978 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:07.978 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/20\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.6945 - accuracy: 0.5462 - val_loss: 0.6177 - val_accuracy: 0.7049\u001b[0m7433 - accuracy: 0.46\n",
      "\u001b[35m2023-05-14 18:37:08.414 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/20\u001b[0m\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5880 - accuracy: 0.7280 - val_loss: 0.5480 - val_accuracy: 0.7447\u001b[0m6124 - accuracy: 0.70\n",
      "\u001b[35m2023-05-14 18:37:08.495 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5283 - accuracy: 0.7506 - val_loss: 0.5279 - val_accuracy: 0.7524\u001b[0m5539 - accuracy: 0.72\n",
      "\u001b[35m2023-05-14 18:37:08.599 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5108 - accuracy: 0.7612 - val_loss: 0.5229 - val_accuracy: 0.7566\u001b[0m5032 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:08.705 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/20\u001b[0m\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7650 - val_loss: 0.5210 - val_accuracy: 0.7579\u001b[0m5172 - accuracy: 0.74\n",
      "\u001b[35m2023-05-14 18:37:08.804 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/20\u001b[0m\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4951 - accuracy: 0.7714 - val_loss: 0.5197 - val_accuracy: 0.7560\u001b[0m4623 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:08.904 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4867 - accuracy: 0.7690 - val_loss: 0.5194 - val_accuracy: 0.7571\u001b[0m4697 - accuracy: 0.76\n",
      "\u001b[35m2023-05-14 18:37:09.006 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/20\u001b[0m\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4881 - accuracy: 0.7711 - val_loss: 0.5183 - val_accuracy: 0.7577\u001b[0m5159 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:37:09.106 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/20\u001b[0m\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4847 - accuracy: 0.7762 - val_loss: 0.5159 - val_accuracy: 0.7577\u001b[0m4713 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:09.205 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4867 - accuracy: 0.7762 - val_loss: 0.5145 - val_accuracy: 0.7596\u001b[0m4832 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:09.307 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 11/20\u001b[0m\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.7761 - val_loss: 0.5151 - val_accuracy: 0.7557\u001b[0m4819 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:09.405 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 12/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4737 - accuracy: 0.7767 - val_loss: 0.5142 - val_accuracy: 0.7615\u001b[0m4693 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:09.507 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 13/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4732 - accuracy: 0.7765 - val_loss: 0.5146 - val_accuracy: 0.7574\u001b[0m4864 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:37:09.609 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 14/20\u001b[0m\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4735 - accuracy: 0.7785 - val_loss: 0.5137 - val_accuracy: 0.7626\u001b[0m4655 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:09.708 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 15/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4703 - accuracy: 0.7790 - val_loss: 0.5138 - val_accuracy: 0.7577\u001b[0m4956 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:37:09.810 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 16/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4639 - accuracy: 0.7872 - val_loss: 0.5119 - val_accuracy: 0.7635\u001b[0m4670 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:09.915 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 17/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4655 - accuracy: 0.7872 - val_loss: 0.5133 - val_accuracy: 0.7632\u001b[0m4561 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:10.019 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 18/20\u001b[0m\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4691 - accuracy: 0.7824 - val_loss: 0.5153 - val_accuracy: 0.7604\u001b[0m4898 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:10.116 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 19/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.7914 - val_loss: 0.5151 - val_accuracy: 0.7590\u001b[0m4521 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:10.240 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 20/20\u001b[0m\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4533 - accuracy: 0.7968 - val_loss: 0.5142 - val_accuracy: 0.7607\u001b[0m4224 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:37:10.327 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:10.567 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:10.567 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:10.807 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:10.807 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:11.540 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:11.540 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/5\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.5939 - accuracy: 0.6913 - val_loss: 0.5117 - val_accuracy: 0.7695\u001b[0m.7150 - accuracy: 0.56\n",
      "\u001b[35m2023-05-14 18:37:12.395 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/5\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4846 - accuracy: 0.7782 - val_loss: 0.5024 - val_accuracy: 0.7759\u001b[0m429 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:37:12.873 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/5\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4643 - accuracy: 0.7888 - val_loss: 0.5020 - val_accuracy: 0.7673\u001b[0m837 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:13.347 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/5\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4558 - accuracy: 0.7977 - val_loss: 0.5035 - val_accuracy: 0.7690\u001b[0m752 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:13.856 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/5\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.7952 - val_loss: 0.4998 - val_accuracy: 0.7737\u001b[0m300 - accuracy: 0.90\n",
      "\u001b[35m2023-05-14 18:37:14.334 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:14.571 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:14.571 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:14.832 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:14.832 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:15.575 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:15.575 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.5865 - accuracy: 0.6989 - val_loss: 0.5100 - val_accuracy: 0.7618\u001b[0m.6861 - accuracy: 0.53\n",
      "\u001b[35m2023-05-14 18:37:16.448 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4896 - accuracy: 0.7772 - val_loss: 0.4988 - val_accuracy: 0.7712\u001b[0m162 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:37:16.939 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4739 - accuracy: 0.7852 - val_loss: 0.5017 - val_accuracy: 0.7715\u001b[0m974 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:17.439 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.7974 - val_loss: 0.4985 - val_accuracy: 0.7728\u001b[0m223 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:17.929 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4537 - accuracy: 0.7941 - val_loss: 0.5013 - val_accuracy: 0.7753\u001b[0m129 - accuracy: 0.90\n",
      "\u001b[35m2023-05-14 18:37:18.415 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4474 - accuracy: 0.7970 - val_loss: 0.5031 - val_accuracy: 0.7750\u001b[0m165 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:18.900 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4339 - accuracy: 0.8102 - val_loss: 0.5117 - val_accuracy: 0.7662\u001b[0m907 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:19.376 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4277 - accuracy: 0.8114 - val_loss: 0.5158 - val_accuracy: 0.7704\u001b[0m463 - accuracy: 0.59\n",
      "\u001b[35m2023-05-14 18:37:19.856 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4178 - accuracy: 0.8215 - val_loss: 0.5263 - val_accuracy: 0.7665\u001b[0m801 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:20.336 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4117 - accuracy: 0.8267 - val_loss: 0.5288 - val_accuracy: 0.7577\u001b[0m607 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:20.816 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:21.063 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:21.063 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:21.325 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:21.325 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:22.075 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:22.076 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.5921 - accuracy: 0.6963 - val_loss: 0.5066 - val_accuracy: 0.7687\u001b[0m.7438 - accuracy: 0.50\n",
      "\u001b[35m2023-05-14 18:37:22.907 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4913 - accuracy: 0.7761 - val_loss: 0.5000 - val_accuracy: 0.7770\u001b[0m582 - accuracy: 0.90\n",
      "\u001b[35m2023-05-14 18:37:23.382 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4742 - accuracy: 0.7835 - val_loss: 0.5012 - val_accuracy: 0.7706\u001b[0m172 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:23.909 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4588 - accuracy: 0.7942 - val_loss: 0.5035 - val_accuracy: 0.7704\u001b[0m270 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:24.384 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4504 - accuracy: 0.7984 - val_loss: 0.5069 - val_accuracy: 0.7690\u001b[0m589 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:37:24.858 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4379 - accuracy: 0.8092 - val_loss: 0.5100 - val_accuracy: 0.7665\u001b[0m522 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:25.335 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4313 - accuracy: 0.8139 - val_loss: 0.5155 - val_accuracy: 0.7640\u001b[0m598 - accuracy: 0.90\n",
      "\u001b[35m2023-05-14 18:37:25.843 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4279 - accuracy: 0.8169 - val_loss: 0.5222 - val_accuracy: 0.7590\u001b[0m738 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:26.339 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.4125 - accuracy: 0.8285 - val_loss: 0.5241 - val_accuracy: 0.7577\u001b[0m780 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:26.825 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.3956 - accuracy: 0.8391 - val_loss: 0.5355 - val_accuracy: 0.7532\u001b[0m611 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:37:27.308 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 11/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.8382 - val_loss: 0.5395 - val_accuracy: 0.7577\u001b[0m467 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:27.788 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 12/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.3744 - accuracy: 0.8478 - val_loss: 0.5522 - val_accuracy: 0.7527\u001b[0m514 - accuracy: 0.96\n",
      "\u001b[35m2023-05-14 18:37:28.265 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 13/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.3722 - accuracy: 0.8511 - val_loss: 0.5613 - val_accuracy: 0.7535\u001b[0m585 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:37:28.753 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 14/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.3650 - accuracy: 0.8549 - val_loss: 0.5747 - val_accuracy: 0.7486\u001b[0m067 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:29.252 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 15/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.3626 - accuracy: 0.8535 - val_loss: 0.5879 - val_accuracy: 0.7450\u001b[0m551 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:37:29.742 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 16/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3645 - accuracy: 0.8549 - val_loss: 0.5994 - val_accuracy: 0.7477\u001b[0m973 - accuracy: 0.96\n",
      "\u001b[35m2023-05-14 18:37:30.247 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 17/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3511 - accuracy: 0.8615 - val_loss: 0.6092 - val_accuracy: 0.7436\u001b[0m284 - accuracy: 0.93\n",
      "\u001b[35m2023-05-14 18:37:30.756 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 18/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.3538 - accuracy: 0.8588 - val_loss: 0.6141 - val_accuracy: 0.7419\u001b[0m934 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:37:31.253 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 19/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.3439 - accuracy: 0.8670 - val_loss: 0.6218 - val_accuracy: 0.7334\u001b[0m656 - accuracy: 0.93\n",
      "\u001b[35m2023-05-14 18:37:31.749 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 20/20\u001b[0m\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.3403 - accuracy: 0.8707 - val_loss: 0.6387 - val_accuracy: 0.7372\u001b[0m904 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:32.239 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:32.483 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:32.483 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:32.771 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:32.771 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:33.486 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:33.486 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/5\u001b[0m\n",
      "57/57 [==============================] - 1s 4ms/step - loss: 0.6549 - accuracy: 0.6267 - val_loss: 0.5365 - val_accuracy: 0.7543\u001b[0m.7072 - accuracy: 0.511\n",
      "\u001b[35m2023-05-14 18:37:34.006 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/5\u001b[0m\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5132 - accuracy: 0.7674 - val_loss: 0.5092 - val_accuracy: 0.7701\u001b[0m5390 - accuracy: 0.74\n",
      "\u001b[35m2023-05-14 18:37:34.143 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/5\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.7838 - val_loss: 0.5046 - val_accuracy: 0.7717\u001b[0m4338 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:37:34.324 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/5\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4782 - accuracy: 0.7854 - val_loss: 0.5038 - val_accuracy: 0.7704\u001b[0m4570 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:34.503 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/5\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4730 - accuracy: 0.7883 - val_loss: 0.5036 - val_accuracy: 0.7701\u001b[0m4368 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:34.672 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:34.984 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:34.984 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:35.229 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:35.229 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:36.026 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:36.026 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6540 - accuracy: 0.6268 - val_loss: 0.5324 - val_accuracy: 0.7566\u001b[0m.6974 - accuracy: 0.554\n",
      "\u001b[35m2023-05-14 18:37:36.527 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.5090 - accuracy: 0.7724 - val_loss: 0.5101 - val_accuracy: 0.7684\u001b[0m5404 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:36.691 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4916 - accuracy: 0.7715 - val_loss: 0.5050 - val_accuracy: 0.7720\u001b[0m5208 - accuracy: 0.74\n",
      "\u001b[35m2023-05-14 18:37:36.842 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4779 - accuracy: 0.7865 - val_loss: 0.5015 - val_accuracy: 0.7723\u001b[0m4649 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:37:37.020 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4730 - accuracy: 0.7835 - val_loss: 0.5009 - val_accuracy: 0.7709\u001b[0m4374 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:37.199 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4706 - accuracy: 0.7843 - val_loss: 0.4997 - val_accuracy: 0.7750\u001b[0m5133 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:37:37.368 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4660 - accuracy: 0.7930 - val_loss: 0.5004 - val_accuracy: 0.7701\u001b[0m4768 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:37.535 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4613 - accuracy: 0.7943 - val_loss: 0.4993 - val_accuracy: 0.7731\u001b[0m4795 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:37.703 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4625 - accuracy: 0.7872 - val_loss: 0.4989 - val_accuracy: 0.7803\u001b[0m4432 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:37.869 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4551 - accuracy: 0.7942 - val_loss: 0.5009 - val_accuracy: 0.7739\u001b[0m4868 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:38.029 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:38.268 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:38.268 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:38.527 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:38.527 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:39.539 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:39.539 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6680 - accuracy: 0.5994 - val_loss: 0.5433 - val_accuracy: 0.7535\u001b[0m.7130 - accuracy: 0.500\n",
      "\u001b[35m2023-05-14 18:37:40.030 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.5106 - accuracy: 0.7716 - val_loss: 0.5102 - val_accuracy: 0.7687\u001b[0m4958 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:40.195 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4844 - accuracy: 0.7815 - val_loss: 0.5049 - val_accuracy: 0.7706\u001b[0m4518 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:40.343 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4755 - accuracy: 0.7851 - val_loss: 0.5020 - val_accuracy: 0.7756\u001b[0m4678 - accuracy: 0.76\n",
      "\u001b[35m2023-05-14 18:37:40.523 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4651 - accuracy: 0.7916 - val_loss: 0.5030 - val_accuracy: 0.7745\u001b[0m4659 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:37:40.702 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4616 - accuracy: 0.7920 - val_loss: 0.5019 - val_accuracy: 0.7720\u001b[0m4143 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:37:40.878 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4552 - accuracy: 0.8000 - val_loss: 0.5007 - val_accuracy: 0.7712\u001b[0m4345 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:41.033 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4563 - accuracy: 0.7947 - val_loss: 0.5006 - val_accuracy: 0.7742\u001b[0m4977 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:37:41.212 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4546 - accuracy: 0.7963 - val_loss: 0.5003 - val_accuracy: 0.7712\u001b[0m4707 - accuracy: 0.76\n",
      "\u001b[35m2023-05-14 18:37:41.399 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4471 - accuracy: 0.8002 - val_loss: 0.5031 - val_accuracy: 0.7706\u001b[0m5118 - accuracy: 0.76\n",
      "\u001b[35m2023-05-14 18:37:41.625 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 11/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4452 - accuracy: 0.8075 - val_loss: 0.5042 - val_accuracy: 0.7706\u001b[0m4757 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:41.802 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 12/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4400 - accuracy: 0.8067 - val_loss: 0.5037 - val_accuracy: 0.7668\u001b[0m4232 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:41.970 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 13/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4335 - accuracy: 0.8083 - val_loss: 0.5057 - val_accuracy: 0.7693\u001b[0m3899 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:42.128 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 14/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4327 - accuracy: 0.8072 - val_loss: 0.5093 - val_accuracy: 0.7654\u001b[0m4382 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:42.308 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 15/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4206 - accuracy: 0.8157 - val_loss: 0.5067 - val_accuracy: 0.7684\u001b[0m3872 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:42.478 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 16/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4236 - accuracy: 0.8196 - val_loss: 0.5099 - val_accuracy: 0.7654\u001b[0m4403 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:37:42.636 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 17/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4139 - accuracy: 0.8238 - val_loss: 0.5126 - val_accuracy: 0.7673\u001b[0m4026 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:37:42.813 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 18/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4047 - accuracy: 0.8275 - val_loss: 0.5154 - val_accuracy: 0.7624\u001b[0m4196 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:43.036 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 19/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4118 - accuracy: 0.8246 - val_loss: 0.5179 - val_accuracy: 0.7646\u001b[0m4022 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:43.202 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 20/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4001 - accuracy: 0.8302 - val_loss: 0.5208 - val_accuracy: 0.7607\u001b[0m3872 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:37:43.360 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:43.612 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:43.612 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:43.878 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:43.878 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:44.621 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:44.621 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/5\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.6802 - accuracy: 0.5751 - val_loss: 0.5926 - val_accuracy: 0.7259\u001b[0m7122 - accuracy: 0.48\n",
      "\u001b[35m2023-05-14 18:37:45.085 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/5\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5605 - accuracy: 0.7480 - val_loss: 0.5289 - val_accuracy: 0.7546\u001b[0m5842 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:37:45.203 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/5\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5014 - accuracy: 0.7752 - val_loss: 0.5120 - val_accuracy: 0.7668\u001b[0m4598 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:45.369 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/5\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4869 - accuracy: 0.7826 - val_loss: 0.5062 - val_accuracy: 0.7709\u001b[0m4866 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:45.498 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/5\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4808 - accuracy: 0.7868 - val_loss: 0.5042 - val_accuracy: 0.7715\u001b[0m5013 - accuracy: 0.76\n",
      "\u001b[35m2023-05-14 18:37:45.622 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:45.907 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:45.907 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:46.165 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:46.165 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:46.909 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:46.909 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.6601 - accuracy: 0.6172 - val_loss: 0.5661 - val_accuracy: 0.7378\u001b[0m6937 - accuracy: 0.54\n",
      "\u001b[35m2023-05-14 18:37:47.383 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5420 - accuracy: 0.7508 - val_loss: 0.5236 - val_accuracy: 0.7610\u001b[0m5918 - accuracy: 0.69\n",
      "\u001b[35m2023-05-14 18:37:47.503 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5018 - accuracy: 0.7715 - val_loss: 0.5129 - val_accuracy: 0.7668\u001b[0m5236 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:37:47.668 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.4848 - accuracy: 0.7814 - val_loss: 0.5049 - val_accuracy: 0.7745\u001b[0m4589 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:47.797 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4799 - accuracy: 0.7817 - val_loss: 0.5050 - val_accuracy: 0.7792\u001b[0m4828 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:47.917 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.4744 - accuracy: 0.7873 - val_loss: 0.5035 - val_accuracy: 0.7734\u001b[0m4623 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:48.170 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4744 - accuracy: 0.7847 - val_loss: 0.5041 - val_accuracy: 0.7773\u001b[0m4573 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:48.299 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4688 - accuracy: 0.7909 - val_loss: 0.5021 - val_accuracy: 0.7759\u001b[0m4469 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:48.416 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4649 - accuracy: 0.7910 - val_loss: 0.5023 - val_accuracy: 0.7728\u001b[0m4357 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:48.591 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4672 - accuracy: 0.7888 - val_loss: 0.5017 - val_accuracy: 0.7770\u001b[0m4517 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:48.711 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:48.969 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:48.969 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:49.242 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:49.242 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:49.960 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:49.960 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/20\u001b[0m\n",
      "29/29 [==============================] - 1s 8ms/step - loss: 0.6672 - accuracy: 0.6073 - val_loss: 0.5773 - val_accuracy: 0.7359\u001b[0m7037 - accuracy: 0.53\n",
      "\u001b[35m2023-05-14 18:37:50.466 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/20\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.5439 - accuracy: 0.7582 - val_loss: 0.5238 - val_accuracy: 0.7651\u001b[0m5790 - accuracy: 0.73\n",
      "\u001b[35m2023-05-14 18:37:50.596 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5050 - accuracy: 0.7710 - val_loss: 0.5112 - val_accuracy: 0.7695\u001b[0m5273 - accuracy: 0.74\n",
      "\u001b[35m2023-05-14 18:37:50.713 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/20\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4901 - accuracy: 0.7774 - val_loss: 0.5072 - val_accuracy: 0.7739\u001b[0m5171 - accuracy: 0.74\n",
      "\u001b[35m2023-05-14 18:37:50.888 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4765 - accuracy: 0.7896 - val_loss: 0.5053 - val_accuracy: 0.7781\u001b[0m4503 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:37:51.007 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/20\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4745 - accuracy: 0.7882 - val_loss: 0.5059 - val_accuracy: 0.7742\u001b[0m4442 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:51.171 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/20\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.4746 - accuracy: 0.7869 - val_loss: 0.5057 - val_accuracy: 0.7715\u001b[0m4496 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:51.302 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4790 - accuracy: 0.7828 - val_loss: 0.5031 - val_accuracy: 0.7778\u001b[0m5294 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:37:51.422 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/20\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.4681 - accuracy: 0.7865 - val_loss: 0.5029 - val_accuracy: 0.7770\u001b[0m4494 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:51.595 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4709 - accuracy: 0.7833 - val_loss: 0.5029 - val_accuracy: 0.7709\u001b[0m4843 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:37:51.711 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 11/20\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4649 - accuracy: 0.7896 - val_loss: 0.5040 - val_accuracy: 0.7723\u001b[0m4676 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:51.884 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 12/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.7948 - val_loss: 0.5010 - val_accuracy: 0.7753\u001b[0m4411 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:52.007 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 13/20\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4573 - accuracy: 0.7956 - val_loss: 0.5013 - val_accuracy: 0.7773\u001b[0m4762 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:52.169 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 14/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4533 - accuracy: 0.7936 - val_loss: 0.5007 - val_accuracy: 0.7792\u001b[0m4492 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:52.296 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 15/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4577 - accuracy: 0.7920 - val_loss: 0.5021 - val_accuracy: 0.7753\u001b[0m4458 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:37:52.416 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 16/20\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4452 - accuracy: 0.7995 - val_loss: 0.5004 - val_accuracy: 0.7756\u001b[0m4110 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:37:52.588 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 17/20\u001b[0m\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4440 - accuracy: 0.8017 - val_loss: 0.5004 - val_accuracy: 0.7778\u001b[0m4286 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:37:52.707 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 18/20\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4428 - accuracy: 0.8007 - val_loss: 0.4998 - val_accuracy: 0.7748\u001b[0m4203 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:37:52.871 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 19/20\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4391 - accuracy: 0.8043 - val_loss: 0.5029 - val_accuracy: 0.7717\u001b[0m4367 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:37:53.031 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 20/20\u001b[0m\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8031 - val_loss: 0.5037 - val_accuracy: 0.7715\u001b[0m4470 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:53.121 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:53.437 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:53.437 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:53.690 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:53.690 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:54.446 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:54.446 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/5\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.5815 - accuracy: 0.7073 - val_loss: 0.5018 - val_accuracy: 0.7753\u001b[0m.7040 - accuracy: 0.50\n",
      "\u001b[35m2023-05-14 18:37:55.347 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/5\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4758 - accuracy: 0.7833 - val_loss: 0.4968 - val_accuracy: 0.7717\u001b[0m882 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:37:55.907 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/5\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4535 - accuracy: 0.7935 - val_loss: 0.4967 - val_accuracy: 0.7750\u001b[0m464 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:37:56.468 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/5\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4491 - accuracy: 0.7996 - val_loss: 0.5030 - val_accuracy: 0.7731\u001b[0m754 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:57.011 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/5\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4293 - accuracy: 0.8081 - val_loss: 0.5058 - val_accuracy: 0.7712\u001b[0m505 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:37:57.558 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:57.891 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:57.892 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:58.161 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:58.161 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:58.942 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:37:58.942 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.5833 - accuracy: 0.7002 - val_loss: 0.5037 - val_accuracy: 0.7737\u001b[0m.6632 - accuracy: 0.53\n",
      "\u001b[35m2023-05-14 18:37:59.837 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4839 - accuracy: 0.7854 - val_loss: 0.5000 - val_accuracy: 0.7654\u001b[0m354 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:38:00.372 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4624 - accuracy: 0.7900 - val_loss: 0.4982 - val_accuracy: 0.7767\u001b[0m807 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:38:00.896 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4439 - accuracy: 0.8014 - val_loss: 0.4992 - val_accuracy: 0.7731\u001b[0m711 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:01.434 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4375 - accuracy: 0.8036 - val_loss: 0.5032 - val_accuracy: 0.7712\u001b[0m569 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:01.964 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4130 - accuracy: 0.8242 - val_loss: 0.5068 - val_accuracy: 0.7681\u001b[0m627 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:02.487 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4186 - accuracy: 0.8224 - val_loss: 0.5147 - val_accuracy: 0.7690\u001b[0m168 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:03.028 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3954 - accuracy: 0.8378 - val_loss: 0.5240 - val_accuracy: 0.7676\u001b[0m014 - accuracy: 0.93\n",
      "\u001b[35m2023-05-14 18:38:03.551 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3948 - accuracy: 0.8427 - val_loss: 0.5348 - val_accuracy: 0.7715\u001b[0m810 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:38:04.078 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3785 - accuracy: 0.8471 - val_loss: 0.5410 - val_accuracy: 0.7632\u001b[0m149 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:38:04.604 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:04.873 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:04.873 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:05.140 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:05.140 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:05.916 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:05.916 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.5869 - accuracy: 0.6912 - val_loss: 0.4969 - val_accuracy: 0.7800\u001b[0m.7006 - accuracy: 0.46\n",
      "\u001b[35m2023-05-14 18:38:06.825 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4799 - accuracy: 0.7810 - val_loss: 0.4982 - val_accuracy: 0.7759\u001b[0m969 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:07.360 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4593 - accuracy: 0.7945 - val_loss: 0.4984 - val_accuracy: 0.7756\u001b[0m307 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:07.897 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4498 - accuracy: 0.8018 - val_loss: 0.5012 - val_accuracy: 0.7778\u001b[0m660 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:08.429 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4347 - accuracy: 0.8111 - val_loss: 0.5081 - val_accuracy: 0.7712\u001b[0m039 - accuracy: 0.90\n",
      "\u001b[35m2023-05-14 18:38:08.959 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4333 - accuracy: 0.8124 - val_loss: 0.5141 - val_accuracy: 0.7715\u001b[0m159 - accuracy: 0.71\n",
      "\u001b[35m2023-05-14 18:38:09.486 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4078 - accuracy: 0.8318 - val_loss: 0.5225 - val_accuracy: 0.7706\u001b[0m318 - accuracy: 0.90\n",
      "\u001b[35m2023-05-14 18:38:10.020 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4044 - accuracy: 0.8307 - val_loss: 0.5285 - val_accuracy: 0.7632\u001b[0m248 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:10.559 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3886 - accuracy: 0.8446 - val_loss: 0.5401 - val_accuracy: 0.7679\u001b[0m100 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:38:11.127 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3797 - accuracy: 0.8478 - val_loss: 0.5492 - val_accuracy: 0.7670\u001b[0m989 - accuracy: 0.93\n",
      "\u001b[35m2023-05-14 18:38:11.684 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 11/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3713 - accuracy: 0.8535 - val_loss: 0.5639 - val_accuracy: 0.7596\u001b[0m289 - accuracy: 0.87\n",
      "\u001b[35m2023-05-14 18:38:12.217 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 12/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3588 - accuracy: 0.8589 - val_loss: 0.5756 - val_accuracy: 0.7621\u001b[0m947 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:12.743 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 13/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3515 - accuracy: 0.8641 - val_loss: 0.5952 - val_accuracy: 0.7563\u001b[0m553 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:13.283 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 14/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3509 - accuracy: 0.8648 - val_loss: 0.6046 - val_accuracy: 0.7516\u001b[0m303 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:38:13.820 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 15/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3254 - accuracy: 0.8813 - val_loss: 0.6238 - val_accuracy: 0.7502\u001b[0m530 - accuracy: 0.93\n",
      "\u001b[35m2023-05-14 18:38:14.356 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 16/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3320 - accuracy: 0.8764 - val_loss: 0.6276 - val_accuracy: 0.7477\u001b[0m146 - accuracy: 0.93\n",
      "\u001b[35m2023-05-14 18:38:14.885 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 17/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3094 - accuracy: 0.8903 - val_loss: 0.6386 - val_accuracy: 0.7480\u001b[0m496 - accuracy: 0.90\n",
      "\u001b[35m2023-05-14 18:38:15.493 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 18/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3024 - accuracy: 0.8919 - val_loss: 0.6498 - val_accuracy: 0.7491\u001b[0m883 - accuracy: 0.96\n",
      "\u001b[35m2023-05-14 18:38:16.059 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 19/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3059 - accuracy: 0.8929 - val_loss: 0.6677 - val_accuracy: 0.7455\u001b[0m668 - accuracy: 0.93\n",
      "\u001b[35m2023-05-14 18:38:16.619 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 20/20\u001b[0m\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.3009 - accuracy: 0.8958 - val_loss: 0.6862 - val_accuracy: 0.7428\u001b[0m919 - accuracy: 0.84\n",
      "\u001b[35m2023-05-14 18:38:17.152 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:17.424 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:17.424 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:17.688 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:17.688 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:18.440 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:18.440 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/5\u001b[0m\n",
      "57/57 [==============================] - 1s 5ms/step - loss: 0.6521 - accuracy: 0.6291 - val_loss: 0.5242 - val_accuracy: 0.7662\u001b[0m.6949 - accuracy: 0.515\n",
      "\u001b[35m2023-05-14 18:38:19.010 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/5\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4979 - accuracy: 0.7785 - val_loss: 0.5034 - val_accuracy: 0.7753\u001b[0m4822 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:19.207 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/5\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4775 - accuracy: 0.7867 - val_loss: 0.4995 - val_accuracy: 0.7822\u001b[0m4707 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:38:19.417 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/5\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4675 - accuracy: 0.7944 - val_loss: 0.4987 - val_accuracy: 0.7828\u001b[0m4599 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:38:19.626 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/5\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4576 - accuracy: 0.7932 - val_loss: 0.4993 - val_accuracy: 0.7773\u001b[0m4248 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:38:19.874 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:20.160 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:20.160 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:20.493 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:20.493 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:21.243 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:21.243 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "57/57 [==============================] - 1s 5ms/step - loss: 0.6544 - accuracy: 0.6319 - val_loss: 0.5317 - val_accuracy: 0.7574\u001b[0m.7046 - accuracy: 0.515\n",
      "\u001b[35m2023-05-14 18:38:21.804 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4969 - accuracy: 0.7796 - val_loss: 0.5046 - val_accuracy: 0.7745\u001b[0m5514 - accuracy: 0.72\n",
      "\u001b[35m2023-05-14 18:38:22.005 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4683 - accuracy: 0.7935 - val_loss: 0.4986 - val_accuracy: 0.7726\u001b[0m4245 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:22.210 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4650 - accuracy: 0.7924 - val_loss: 0.4988 - val_accuracy: 0.7797\u001b[0m4383 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:22.421 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4549 - accuracy: 0.7989 - val_loss: 0.4978 - val_accuracy: 0.7800\u001b[0m4478 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:22.630 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4463 - accuracy: 0.8000 - val_loss: 0.4977 - val_accuracy: 0.7778\u001b[0m4493 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:22.875 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4522 - accuracy: 0.7985 - val_loss: 0.4990 - val_accuracy: 0.7778\u001b[0m4088 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:38:23.095 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4499 - accuracy: 0.8042 - val_loss: 0.4998 - val_accuracy: 0.7803\u001b[0m4316 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:38:23.307 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4343 - accuracy: 0.8069 - val_loss: 0.4968 - val_accuracy: 0.7806\u001b[0m3888 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:38:23.515 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4383 - accuracy: 0.8088 - val_loss: 0.4989 - val_accuracy: 0.7767\u001b[0m4156 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:38:23.733 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:24.054 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:24.054 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:24.572 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:24.572 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:25.237 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:25.237 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/20\u001b[0m\n",
      "57/57 [==============================] - 1s 5ms/step - loss: 0.6372 - accuracy: 0.6504 - val_loss: 0.5173 - val_accuracy: 0.7684\u001b[0m.6906 - accuracy: 0.550\n",
      "\u001b[35m2023-05-14 18:38:25.820 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/20\u001b[0m\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4917 - accuracy: 0.7759 - val_loss: 0.5000 - val_accuracy: 0.7775\u001b[0m4756 - accuracy: 0.76\n",
      "\u001b[35m2023-05-14 18:38:26.006 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4711 - accuracy: 0.7840 - val_loss: 0.4995 - val_accuracy: 0.7800\u001b[0m4547 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:26.215 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/20\u001b[0m\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4616 - accuracy: 0.7915 - val_loss: 0.4980 - val_accuracy: 0.7814\u001b[0m4608 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:26.509 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4480 - accuracy: 0.8000 - val_loss: 0.4950 - val_accuracy: 0.7784\u001b[0m4359 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:26.790 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4493 - accuracy: 0.7963 - val_loss: 0.4976 - val_accuracy: 0.7781\u001b[0m4230 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:26.996 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4377 - accuracy: 0.8086 - val_loss: 0.4970 - val_accuracy: 0.7778\u001b[0m4396 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:38:27.208 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4361 - accuracy: 0.8012 - val_loss: 0.4973 - val_accuracy: 0.7748\u001b[0m3912 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:38:27.421 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4298 - accuracy: 0.8092 - val_loss: 0.4966 - val_accuracy: 0.7742\u001b[0m4027 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:27.666 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4313 - accuracy: 0.8113 - val_loss: 0.5003 - val_accuracy: 0.7770\u001b[0m3834 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:38:27.876 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 11/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4269 - accuracy: 0.8144 - val_loss: 0.5016 - val_accuracy: 0.7731\u001b[0m4551 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:28.093 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 12/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4093 - accuracy: 0.8195 - val_loss: 0.5013 - val_accuracy: 0.7742\u001b[0m3988 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:28.303 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 13/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4071 - accuracy: 0.8243 - val_loss: 0.5042 - val_accuracy: 0.7701\u001b[0m3745 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:38:28.510 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 14/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4140 - accuracy: 0.8210 - val_loss: 0.5039 - val_accuracy: 0.7737\u001b[0m4501 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:38:28.716 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 15/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4001 - accuracy: 0.8257 - val_loss: 0.5058 - val_accuracy: 0.7712\u001b[0m4248 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:28.929 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 16/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3838 - accuracy: 0.8391 - val_loss: 0.5097 - val_accuracy: 0.7745\u001b[0m3809 - accuracy: 0.85\n",
      "\u001b[35m2023-05-14 18:38:29.172 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 17/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3883 - accuracy: 0.8398 - val_loss: 0.5092 - val_accuracy: 0.7715\u001b[0m4100 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:29.384 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 18/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3824 - accuracy: 0.8398 - val_loss: 0.5127 - val_accuracy: 0.7687\u001b[0m3898 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:38:29.596 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 19/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3721 - accuracy: 0.8451 - val_loss: 0.5191 - val_accuracy: 0.7662\u001b[0m3601 - accuracy: 0.83\n",
      "\u001b[35m2023-05-14 18:38:29.804 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 20/20\u001b[0m\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3708 - accuracy: 0.8487 - val_loss: 0.5202 - val_accuracy: 0.7662\u001b[0m3550 - accuracy: 0.85\n",
      "\u001b[35m2023-05-14 18:38:30.015 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:30.288 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:30.288 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:30.552 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:30.552 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:31.327 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:31.328 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/5\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.6630 - accuracy: 0.6272 - val_loss: 0.5611 - val_accuracy: 0.7588\u001b[0m6971 - accuracy: 0.56\n",
      "\u001b[35m2023-05-14 18:38:31.812 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/5\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5248 - accuracy: 0.7741 - val_loss: 0.5108 - val_accuracy: 0.7762\u001b[0m5268 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:31.970 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/5\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.4797 - accuracy: 0.7857 - val_loss: 0.5022 - val_accuracy: 0.7748\u001b[0m4709 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:32.111 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/5\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4680 - accuracy: 0.7957 - val_loss: 0.5006 - val_accuracy: 0.7784\u001b[0m4650 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:32.306 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/5\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4616 - accuracy: 0.7915 - val_loss: 0.5004 - val_accuracy: 0.7795\u001b[0m4495 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:32.499 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:32.773 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:32.773 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:33.059 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:33.060 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:33.837 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:33.837 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.6718 - accuracy: 0.6035 - val_loss: 0.5695 - val_accuracy: 0.7488\u001b[0m7141 - accuracy: 0.50\n",
      "\u001b[35m2023-05-14 18:38:34.317 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.5378 - accuracy: 0.7610 - val_loss: 0.5123 - val_accuracy: 0.7759\u001b[0m5466 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:34.472 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.4896 - accuracy: 0.7825 - val_loss: 0.5022 - val_accuracy: 0.7759\u001b[0m5056 - accuracy: 0.77\n",
      "\u001b[35m2023-05-14 18:38:34.616 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4681 - accuracy: 0.7942 - val_loss: 0.4997 - val_accuracy: 0.7789\u001b[0m4538 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:34.803 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4658 - accuracy: 0.7900 - val_loss: 0.4985 - val_accuracy: 0.7831\u001b[0m4649 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:34.994 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4441 - accuracy: 0.8069 - val_loss: 0.4991 - val_accuracy: 0.7806\u001b[0m4559 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:35.185 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4464 - accuracy: 0.8034 - val_loss: 0.5006 - val_accuracy: 0.7781\u001b[0m4544 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:35.369 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.4470 - accuracy: 0.7984 - val_loss: 0.5004 - val_accuracy: 0.7808\u001b[0m4352 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:38:35.507 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4400 - accuracy: 0.8000 - val_loss: 0.5018 - val_accuracy: 0.7753\u001b[0m4235 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:35.703 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4362 - accuracy: 0.8079 - val_loss: 0.5008 - val_accuracy: 0.7778\u001b[0m4510 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:38:35.893 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:36.175 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:36.175 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:36.431 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:36.431 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(18112,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:37.207 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mDress just arrived and i was a bit nervous of the fit based on the other six reviewers stating it runs small. i actually found it to run exactly true to size. i'm 5'6\" 38-26-38 slender, and on average i wear a size 6, which is what i ordered - 6 regular and it fits as though it was custom made for me. this dress is made very well - looks tailored and hits me right above the knee similar to the model. i feel i scored big by purchasing it at the sale price.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:37.208 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 1/20\u001b[0m\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.6713 - accuracy: 0.6036 - val_loss: 0.5751 - val_accuracy: 0.7519\u001b[0m7100 - accuracy: 0.50\n",
      "\u001b[35m2023-05-14 18:38:37.697 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 2/20\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5428 - accuracy: 0.7632 - val_loss: 0.5148 - val_accuracy: 0.7709\u001b[0m5739 - accuracy: 0.75\n",
      "\u001b[35m2023-05-14 18:38:37.865 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 3/20\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.4885 - accuracy: 0.7792 - val_loss: 0.5013 - val_accuracy: 0.7789\u001b[0m4658 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:38:38.008 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 4/20\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4681 - accuracy: 0.7905 - val_loss: 0.4979 - val_accuracy: 0.7762\u001b[0m4692 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:38:38.208 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 5/20\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4579 - accuracy: 0.7991 - val_loss: 0.4984 - val_accuracy: 0.7784\u001b[0m4392 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:38.407 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 6/20\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4604 - accuracy: 0.7938 - val_loss: 0.4963 - val_accuracy: 0.7792\u001b[0m4633 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:38:38.600 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 7/20\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4556 - accuracy: 0.8008 - val_loss: 0.4969 - val_accuracy: 0.7797\u001b[0m4565 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:38.809 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 8/20\u001b[0m\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4536 - accuracy: 0.7978 - val_loss: 0.4961 - val_accuracy: 0.7819\u001b[0m4750 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:38.972 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 9/20\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.4520 - accuracy: 0.7979 - val_loss: 0.4973 - val_accuracy: 0.7814\u001b[0m4646 - accuracy: 0.78\n",
      "\u001b[35m2023-05-14 18:38:39.112 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 10/20\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4446 - accuracy: 0.8014 - val_loss: 0.4958 - val_accuracy: 0.7828\u001b[0m4361 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:39.303 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 11/20\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4378 - accuracy: 0.8051 - val_loss: 0.4972 - val_accuracy: 0.7806\u001b[0m4230 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:39.499 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 12/20\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4406 - accuracy: 0.8050 - val_loss: 0.4965 - val_accuracy: 0.7808\u001b[0m4550 - accuracy: 0.79\n",
      "\u001b[35m2023-05-14 18:38:39.698 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 13/20\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4455 - accuracy: 0.8010 - val_loss: 0.4966 - val_accuracy: 0.7817\u001b[0m4893 - accuracy: 0.76\n",
      "\u001b[35m2023-05-14 18:38:39.894 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 14/20\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4304 - accuracy: 0.8069 - val_loss: 0.4973 - val_accuracy: 0.7792\u001b[0m4092 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:40.092 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 15/20\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4230 - accuracy: 0.8134 - val_loss: 0.4965 - val_accuracy: 0.7828\u001b[0m4264 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:40.288 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 16/20\u001b[0m\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.4262 - accuracy: 0.8132 - val_loss: 0.4974 - val_accuracy: 0.7831\u001b[0m4286 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:40.514 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 17/20\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4270 - accuracy: 0.8145 - val_loss: 0.4971 - val_accuracy: 0.7825\u001b[0m4513 - accuracy: 0.81\n",
      "\u001b[35m2023-05-14 18:38:40.773 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 18/20\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.4200 - accuracy: 0.8141 - val_loss: 0.4970 - val_accuracy: 0.7822\u001b[0m4267 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:40.915 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 19/20\u001b[0m\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4142 - accuracy: 0.8157 - val_loss: 0.4994 - val_accuracy: 0.7775\u001b[0m4231 - accuracy: 0.80\n",
      "\u001b[35m2023-05-14 18:38:41.164 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mEpoch 20/20\u001b[0m\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.4068 - accuracy: 0.8248 - val_loss: 0.5005 - val_accuracy: 0.7786\u001b[0m4100 - accuracy: 0.82\n",
      "\u001b[35m2023-05-14 18:38:41.319 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:41.595 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:41.595 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m(4529,)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:44.840 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22m2023-05-14 18:36:01.055478: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2499995000 Hz\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:44.841 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[22mSoft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:45.116 \u001b[0m\u001b[32m[26/model/141 (pid 17415)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:45.731 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.919 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='Baseline', params='Always predict the majority class: 1', pathspec='BaselineChallenge/26/baseline/140', acc=0.5577390152351512, rocauc=0.5)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.969 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [100, 32, 5]', params={'vocab_sz': 100, 'batch_size': 32, 'epochs': 5}, pathspec='BaselineChallenge/26/model/141', acc=0.7171561051004637, rocauc=0.7771027544194397)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.970 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [100, 32, 10]', params={'vocab_sz': 100, 'batch_size': 32, 'epochs': 10}, pathspec='BaselineChallenge/26/model/141', acc=0.7103113270037535, rocauc=0.7735086799729147)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.970 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [100, 32, 20]', params={'vocab_sz': 100, 'batch_size': 32, 'epochs': 20}, pathspec='BaselineChallenge/26/model/141', acc=0.7105321262971959, rocauc=0.7718094275846722)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.970 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [100, 256, 5]', params={'vocab_sz': 100, 'batch_size': 256, 'epochs': 5}, pathspec='BaselineChallenge/26/model/141', acc=0.7145065135791565, rocauc=0.7741838350945474)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.970 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [100, 256, 10]', params={'vocab_sz': 100, 'batch_size': 256, 'epochs': 10}, pathspec='BaselineChallenge/26/model/141', acc=0.7167145065135792, rocauc=0.7767355301173339)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.970 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [100, 256, 20]', params={'vocab_sz': 100, 'batch_size': 256, 'epochs': 20}, pathspec='BaselineChallenge/26/model/141', acc=0.7138441156988298, rocauc=0.7780911767740314)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.970 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [100, 512, 5]', params={'vocab_sz': 100, 'batch_size': 512, 'epochs': 5}, pathspec='BaselineChallenge/26/model/141', acc=0.7160521086332524, rocauc=0.7741525083712516)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.970 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [100, 512, 10]', params={'vocab_sz': 100, 'batch_size': 512, 'epochs': 10}, pathspec='BaselineChallenge/26/model/141', acc=0.7167145065135792, rocauc=0.7747094718176101)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [100, 512, 20]', params={'vocab_sz': 100, 'batch_size': 512, 'epochs': 20}, pathspec='BaselineChallenge/26/model/141', acc=0.7136233164053875, rocauc=0.7759967333243998)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [300, 32, 5]', params={'vocab_sz': 300, 'batch_size': 32, 'epochs': 5}, pathspec='BaselineChallenge/26/model/141', acc=0.758887171561051, rocauc=0.8298243450343092)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [300, 32, 10]', params={'vocab_sz': 300, 'batch_size': 32, 'epochs': 10}, pathspec='BaselineChallenge/26/model/141', acc=0.746964009715169, rocauc=0.8207008568698813)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [300, 32, 20]', params={'vocab_sz': 300, 'batch_size': 32, 'epochs': 20}, pathspec='BaselineChallenge/26/model/141', acc=0.7410024287922279, rocauc=0.8064380072804491)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [300, 256, 5]', params={'vocab_sz': 300, 'batch_size': 256, 'epochs': 5}, pathspec='BaselineChallenge/26/model/141', acc=0.7549127842790903, rocauc=0.8267422698098537)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [300, 256, 10]', params={'vocab_sz': 300, 'batch_size': 256, 'epochs': 10}, pathspec='BaselineChallenge/26/model/141', acc=0.7577831750938397, rocauc=0.8280714320443325)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [300, 256, 20]', params={'vocab_sz': 300, 'batch_size': 256, 'epochs': 20}, pathspec='BaselineChallenge/26/model/141', acc=0.749613601236476, rocauc=0.826433844877972)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [300, 512, 5]', params={'vocab_sz': 300, 'batch_size': 512, 'epochs': 5}, pathspec='BaselineChallenge/26/model/141', acc=0.7573415765069552, rocauc=0.8258931871393227)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [300, 512, 10]', params={'vocab_sz': 300, 'batch_size': 512, 'epochs': 10}, pathspec='BaselineChallenge/26/model/141', acc=0.7555751821594171, rocauc=0.8268705413771662)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [300, 512, 20]', params={'vocab_sz': 300, 'batch_size': 512, 'epochs': 20}, pathspec='BaselineChallenge/26/model/141', acc=0.758887171561051, rocauc=0.8262750371671314)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [500, 32, 5]', params={'vocab_sz': 500, 'batch_size': 32, 'epochs': 5}, pathspec='BaselineChallenge/26/model/141', acc=0.7672775447118569, rocauc=0.8373591631555042)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [500, 32, 10]', params={'vocab_sz': 500, 'batch_size': 32, 'epochs': 10}, pathspec='BaselineChallenge/26/model/141', acc=0.7624199602561271, rocauc=0.8311266275566855)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [500, 32, 20]', params={'vocab_sz': 500, 'batch_size': 32, 'epochs': 20}, pathspec='BaselineChallenge/26/model/141', acc=0.7396776330315743, rocauc=0.8035573322518201)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [500, 256, 5]', params={'vocab_sz': 500, 'batch_size': 256, 'epochs': 5}, pathspec='BaselineChallenge/26/model/141', acc=0.7776551115036432, rocauc=0.8374176660583155)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.971 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [500, 256, 10]', params={'vocab_sz': 500, 'batch_size': 256, 'epochs': 10}, pathspec='BaselineChallenge/26/model/141', acc=0.7767719143298741, rocauc=0.8398463271047505)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.972 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [500, 256, 20]', params={'vocab_sz': 500, 'batch_size': 256, 'epochs': 20}, pathspec='BaselineChallenge/26/model/141', acc=0.7708103334069331, rocauc=0.8335135855203735)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.972 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [500, 512, 5]', params={'vocab_sz': 500, 'batch_size': 512, 'epochs': 5}, pathspec='BaselineChallenge/26/model/141', acc=0.7694855376462796, rocauc=0.8353453588421802)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:48.972 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [500, 512, 10]', params={'vocab_sz': 500, 'batch_size': 512, 'epochs': 10}, pathspec='BaselineChallenge/26/model/141', acc=0.7697063369397218, rocauc=0.8365699669023781)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:52.749 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [500, 512, 20]', params={'vocab_sz': 500, 'batch_size': 512, 'epochs': 20}, pathspec='BaselineChallenge/26/model/141', acc=0.7758887171561051, rocauc=0.8392842248899018)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:52.749 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [700, 32, 5]', params={'vocab_sz': 700, 'batch_size': 32, 'epochs': 5}, pathspec='BaselineChallenge/26/model/141', acc=0.7794215058511813, rocauc=0.8428278010537638)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:52.749 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [700, 32, 10]', params={'vocab_sz': 700, 'batch_size': 32, 'epochs': 10}, pathspec='BaselineChallenge/26/model/141', acc=0.7745639213954515, rocauc=0.8348453171390975)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:52.749 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [700, 32, 20]', params={'vocab_sz': 700, 'batch_size': 32, 'epochs': 20}, pathspec='BaselineChallenge/26/model/141', acc=0.7482888054758224, rocauc=0.8136724050899107)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:52.749 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [700, 256, 5]', params={'vocab_sz': 700, 'batch_size': 256, 'epochs': 5}, pathspec='BaselineChallenge/26/model/141', acc=0.7825126959593729, rocauc=0.8457543297089204)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:52.749 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [700, 256, 10]', params={'vocab_sz': 700, 'batch_size': 256, 'epochs': 10}, pathspec='BaselineChallenge/26/model/141', acc=0.7811879001987193, rocauc=0.8439560572047706)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:52.749 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [700, 256, 20]', params={'vocab_sz': 700, 'batch_size': 256, 'epochs': 20}, pathspec='BaselineChallenge/26/model/141', acc=0.7745639213954515, rocauc=0.840694322728101)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:52.749 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [700, 512, 5]', params={'vocab_sz': 700, 'batch_size': 512, 'epochs': 5}, pathspec='BaselineChallenge/26/model/141', acc=0.7820710973724884, rocauc=0.8415990424497855)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:52.749 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [700, 512, 10]', params={'vocab_sz': 700, 'batch_size': 512, 'epochs': 10}, pathspec='BaselineChallenge/26/model/141', acc=0.7796423051446235, rocauc=0.8438678087381991)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:52.749 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[22mModelResult(name='NbowModel (vocab, batch, epoch) - [700, 512, 20]', params={'vocab_sz': 700, 'batch_size': 512, 'epochs': 20}, pathspec='BaselineChallenge/26/model/141', acc=0.7827334952528152, rocauc=0.8472080082568151)\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:52.996 \u001b[0m\u001b[32m[26/aggregate/142 (pid 19348)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:53.435 \u001b[0m\u001b[32m[26/end/143 (pid 19449)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:55.919 \u001b[0m\u001b[32m[26/end/143 (pid 19449)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2023-05-14 18:38:56.048 \u001b[0m\u001b[1mDone! See the run in the UI at https://ui-pw-1410177943.outerbounds.dev/BaselineChallenge/26\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python baseline_challenge.py run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full-stack-metaflow-corise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
